<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 13 Patient Level Prediction | The Book of OHDSI</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 13 Patient Level Prediction | The Book of OHDSI" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ohdsi.github.io/TheBookOfOhdsi/" />
  <meta property="og:image" content="https://ohdsi.github.io/TheBookOfOhdsi/images/cover.png" />
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="OHDSI/TheBookOfOhdsi" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Patient Level Prediction | The Book of OHDSI" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="twitter:image" content="https://ohdsi.github.io/TheBookOfOhdsi/images/cover.png" />

<meta name="author" content="Observational Health Data Science and Informatics">


<meta name="date" content="2019-01-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="PopulationLevelEstimation.html">
<link rel="next" href="StudySteps.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-104086677-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-104086677-2');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Book of OHDSI</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I The OHDSI Community</b></span></li>
<li class="chapter" data-level="1" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html"><i class="fa fa-check"></i><b>1</b> Mission, vision, values</a><ul>
<li class="chapter" data-level="1.1" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html#our-mission"><i class="fa fa-check"></i><b>1.1</b> Our Mission</a></li>
<li class="chapter" data-level="1.2" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html#our-vision"><i class="fa fa-check"></i><b>1.2</b> Our Vision</a></li>
<li class="chapter" data-level="1.3" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html#our-objectives"><i class="fa fa-check"></i><b>1.3</b> Our Objectives</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Collaborators.html"><a href="Collaborators.html"><i class="fa fa-check"></i><b>2</b> Collaborators</a></li>
<li class="chapter" data-level="3" data-path="OpenScience.html"><a href="OpenScience.html"><i class="fa fa-check"></i><b>3</b> Open Science</a></li>
<li class="part"><span><b>II Uniform Data Representation</b></span></li>
<li class="chapter" data-level="4" data-path="CommonDataModel.html"><a href="CommonDataModel.html"><i class="fa fa-check"></i><b>4</b> The Common Data Model</a></li>
<li class="chapter" data-level="5" data-path="StandardizedVocabularies.html"><a href="StandardizedVocabularies.html"><i class="fa fa-check"></i><b>5</b> Standardized Vocabularies</a></li>
<li class="chapter" data-level="6" data-path="ExtractTransformLoad.html"><a href="ExtractTransformLoad.html"><i class="fa fa-check"></i><b>6</b> Extract Transform Load</a></li>
<li class="chapter" data-level="7" data-path="DataQuality.html"><a href="DataQuality.html"><i class="fa fa-check"></i><b>7</b> Data Quality</a></li>
<li class="part"><span><b>III Data Analytics</b></span></li>
<li class="chapter" data-level="8" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html"><i class="fa fa-check"></i><b>8</b> Data Analytics Use Cases</a></li>
<li class="chapter" data-level="9" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html"><i class="fa fa-check"></i><b>9</b> OHDSI Analytics Tools</a></li>
<li class="chapter" data-level="10" data-path="Cohorts.html"><a href="Cohorts.html"><i class="fa fa-check"></i><b>10</b> Building the building blocks: cohorts</a></li>
<li class="chapter" data-level="11" data-path="Characterization.html"><a href="Characterization.html"><i class="fa fa-check"></i><b>11</b> Characterization</a></li>
<li class="chapter" data-level="12" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html"><i class="fa fa-check"></i><b>12</b> Population-level estimation</a></li>
<li class="chapter" data-level="13" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html"><i class="fa fa-check"></i><b>13</b> Patient Level Prediction</a><ul>
<li class="chapter" data-level="13.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-specification"><i class="fa fa-check"></i><b>13.1</b> Study specification</a><ul>
<li class="chapter" data-level="13.1.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#problem-definition"><i class="fa fa-check"></i><b>13.1.1</b> Problem definition</a></li>
<li class="chapter" data-level="13.1.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-population-definition"><i class="fa fa-check"></i><b>13.1.2</b> Study population definition</a></li>
<li class="chapter" data-level="13.1.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#model-development-settings"><i class="fa fa-check"></i><b>13.1.3</b> Model development settings</a></li>
<li class="chapter" data-level="13.1.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#model-evaluation"><i class="fa fa-check"></i><b>13.1.4</b> Model evaluation</a></li>
<li class="chapter" data-level="13.1.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-summary"><i class="fa fa-check"></i><b>13.1.5</b> Study summary</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-implementation"><i class="fa fa-check"></i><b>13.2</b> Study implementation</a><ul>
<li class="chapter" data-level="13.2.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#cohort-instantiation"><i class="fa fa-check"></i><b>13.2.1</b> Cohort instantiation</a></li>
<li class="chapter" data-level="13.2.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-script-creation"><i class="fa fa-check"></i><b>13.2.2</b> Study script creation</a></li>
<li class="chapter" data-level="13.2.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-package-creation"><i class="fa fa-check"></i><b>13.2.3</b> Study package creation</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#internal-validation"><i class="fa fa-check"></i><b>13.3</b> Internal validation</a><ul>
<li class="chapter" data-level="13.3.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#discrimination"><i class="fa fa-check"></i><b>13.3.1</b> Discrimination</a></li>
<li class="chapter" data-level="13.3.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#calibration"><i class="fa fa-check"></i><b>13.3.2</b> Calibration</a></li>
<li class="chapter" data-level="13.3.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#smooth-calibration"><i class="fa fa-check"></i><b>13.3.3</b> Smooth Calibration</a></li>
<li class="chapter" data-level="13.3.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#preference-distribution"><i class="fa fa-check"></i><b>13.3.4</b> Preference distribution</a></li>
<li class="chapter" data-level="13.3.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#predicted-probability-distribution"><i class="fa fa-check"></i><b>13.3.5</b> Predicted probability distribution</a></li>
<li class="chapter" data-level="13.3.6" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#test-train-similarity"><i class="fa fa-check"></i><b>13.3.6</b> Test-Train similarity</a></li>
<li class="chapter" data-level="13.3.7" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#variable-scatter-plot"><i class="fa fa-check"></i><b>13.3.7</b> Variable scatter plot</a></li>
<li class="chapter" data-level="13.3.8" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#precision-recall"><i class="fa fa-check"></i><b>13.3.8</b> Precision recall</a></li>
<li class="chapter" data-level="13.3.9" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#demographic-summary"><i class="fa fa-check"></i><b>13.3.9</b> Demographic summary</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#external-validation"><i class="fa fa-check"></i><b>13.4</b> External validation</a></li>
<li class="chapter" data-level="13.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#journal-paper-generation"><i class="fa fa-check"></i><b>13.5</b> Journal paper generation</a></li>
<li class="chapter" data-level="13.6" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#other-functionality"><i class="fa fa-check"></i><b>13.6</b> Other functionality</a></li>
<li class="chapter" data-level="13.7" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#demos"><i class="fa fa-check"></i><b>13.7</b> Demos</a></li>
<li class="chapter" data-level="13.8" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#acknowledgments"><i class="fa fa-check"></i><b>13.8</b> Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#appendix-1-study-population-settings-details"><i class="fa fa-check"></i>Appendix 1: Study population settings details</a></li>
</ul></li>
<li class="part"><span><b>IV OHDSI Studies</b></span></li>
<li class="chapter" data-level="14" data-path="StudySteps.html"><a href="StudySteps.html"><i class="fa fa-check"></i><b>14</b> Study steps</a></li>
<li class="chapter" data-level="15" data-path="NetworkResearch.html"><a href="NetworkResearch.html"><i class="fa fa-check"></i><b>15</b> OHDSI Network Research</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="Glossary.html"><a href="Glossary.html"><i class="fa fa-check"></i><b>A</b> Glossary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Book of OHDSI</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="PatientLevelPrediction" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Patient Level Prediction</h1>
<p>Clinical decision making is a complicated task in which the clinician has to infer a diagnosis or treatment pathway based on the available medical history of the patient and the current clinical guidelines. Clinical prediction models have been developed to support this decision making process and are used in clinical practice in a wide spectrum of specialties. These models predict a diagnostic or prognostic outcome based on a combination of patient characteristics, e.g. demographic information, disease history, treatment history. The number of publications describing clinical prediction models has increased strongly over the last 10 years. An example is the Garvan model that predicts the 5-years and 10-years fractures risk in any elderly man or woman based on age, fracture history, fall history, bone mass density or weight <span class="citation">(N. D. Nguyen et al. <a href="#ref-nguyen2008">2008</a>)</span>. Many prediction models have been developed in patient subgroups at higher risk that need more intensive monitoring, e.g. the prediction of 30-day mortality after an acute myocardial described by <span class="citation">Lee et al. (<a href="#ref-lee1995">1995</a>)</span>. Also, many models have been developed for asymptomatic subjects in the population, e.g. the famous Framingham risk functions for cardiovascular disease <span class="citation">(Wilson et al. <a href="#ref-wilson1998">1998</a>)</span>, or the models for breast cancer screening <span class="citation">(Engel and Fischer <a href="#ref-engel2015">2015</a>)</span>.</p>
<p>Surprisingly, most currently used models are estimated using small datasets and contain a limited set of patient characteristics. For example, in a review of 102 prognostic models in traumatic brain injury showed that three quarters of the models were based on samples with less than 500 patients <span class="citation">(Perel et al. <a href="#ref-perel2006">2006</a>)</span>. This low sample size, and thus low statistical power, forces the data analyst to make stronger modelling assumptions. The selection of the often limited set of patient characteristics is strongly guided by the expert knowledge at hand. This contrasts sharply with the reality of modern medicine wherein patients generate a rich digital trail, which is well beyond the power of any medical practitioner to fully assimilate. Presently, health care is generating huge amount of patient-specific information contained in the Electronic Health Record (EHR). This includes structured data in the form of diagnose, medication, laboratory test results, and unstructured data contained in clinical narratives. Currently, it is unknown how much predictive accuracy can be gained by leveraging the large amount of data originating from the complete EHR of a patient.</p>
<p>Massive-scale, patient-specific predictive modeling has become reality due the OHDSI initiative in which the common data model (CDM) allows for uniform and transparent analysis at an unprecedented scale. These large standardized populations contain rich data to build highly predictive large-scale models and also provide immediate opportunity to serve large communities of patients who are in most need of improved quality of care. Such models can inform truly personalized medical care leading hopefully to sharply improved patient outcomes. Furthermore, these models could assist in the design and analysis of randomized controlled trials (RCT) by enabling a better patient stratification or can be utilized to adjust for confounding variables in observational research. More accurate prediction models contribute to targeting of treatment and to increasing cost-effectiveness of medical care.</p>
<p>Advances in machine learning for large dataset analysis have led to increased interest in applying patient-level prediction on this type of data. However, many published efforts in patient-level-prediction do not follow the model development guidelines, fail to perform extensive external validation, or provide insufficient model details that limits the ability of independent researchers to reproduce the models and perform external validation. This makes it hard to fairly evaluate the predictive performance of the models and reduces the likelihood of the model being used appropriately in clinical practice. To improve standards, several papers have been written detailing guidelines for best practices in developing and reporting prediction models.</p>
<p>The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement (<a href="https://www.equator-network.org/reporting-guidelines/tripod-statement/" class="uri">https://www.equator-network.org/reporting-guidelines/tripod-statement/</a>) provides clear recommendations for reporting prediction model development and validation and addresses some of the concerns related to transparency. However, data structure heterogeneity and inconsistent terminologies still make collaboration and model sharing difficult as different researchers are often required to write new code to extract the data from their databases and may define variables differently.</p>
<p>In our paper <span class="citation">(Reps et al. <a href="#ref-reps2018">2018</a>)</span>, we propose a standardised framework for patient-level prediction that utilizes the OMOP Common Data Model (CDM) and standardized vocabularies, and describe the open-source software that we developed implementing the framework’s pipeline. The framework is the first to support existing best practice guidelines and will enable open dissemination of models that can be extensively validated across the network of OHDSI collaborators.</p>
<p>Figure <a href="PatientLevelPrediction.html#fig:figure1">13.1</a>, illustrates the prediction problem we address. Among a population at risk, we aim to predict which patients at a defined moment in time (t = 0) will experience some outcome during a time-at-risk. Prediction is done using only information about the patients in an observation window prior to that moment in time.</p>
<div class="figure"><span id="fig:figure1"></span>
<img src="images/PatientLevelPrediction/Figure1.png" alt="The prediction problem" width="100%" />
<p class="caption">
Figure 13.1: The prediction problem
</p>
</div>
<p>As shown in Figure <a href="PatientLevelPrediction.html#fig:studydesign">13.2</a>, to define a prediction problem we have to define t=0 by a Target Cohort (T), the outcome we like to predict by an outcome cohort (O), and the time-at-risk (TAR). Furthermore, we have to make design choices for the model we like to develop, and determine the observational datasets to perform internal and external validation. This conceptual framework works for all type of prediction problems, for example those presented in Figure <a href="PatientLevelPrediction.html#fig:problems">13.3</a>.</p>
<div class="figure"><span id="fig:studydesign"></span>
<img src="images/PatientLevelPrediction/studydesign.png" alt="Design choices" width="100%" />
<p class="caption">
Figure 13.2: Design choices
</p>
</div>
<div class="figure"><span id="fig:problems"></span>
<img src="images/PatientLevelPrediction/problems.png" alt="Examples of prediction problems" width="100%" />
<p class="caption">
Figure 13.3: Examples of prediction problems
</p>
</div>
<p>In the next sections we will explain the best practices for model specification, implementation, and evaluation using OHDSI’s Patient-Level Prediction (PLP) framework as guidance.</p>
<div id="study-specification" class="section level2">
<h2><span class="header-section-number">13.1</span> Study specification</h2>
<p>The first step is to clearly define the prediction problem. Interestingly, in many published papers the prediction problem is poorly defined, e.g., it is unclear how the index date (start of the Target Cohort) is exactly defined. A poorly defined prediction problem does not allow for external validation by others let alone implementation in clinical practice. In the PLP framework we have enforced that we have to define the prediction problem we like to address, in which population we will build the model, which model we will build and how we will evaluate its performance. In this section we will guide you through this process and we will use a “Disease onset and progression” prediction type as an example.</p>
<div id="problem-definition" class="section level3">
<h3><span class="header-section-number">13.1.1</span> Problem definition</h3>
<p>Atrial fibrillation is a disease characterized by an irregular heart rate that can cause poor blood flow. Patients with atrial fibrillation are at increased risk of ischemic stroke. Anticoagulation is a recommended prophylaxis treatment strategy for patients at high risk of stroke, though the underuse of anticoagulants and persistent severity of ischemic stroke represents a substantial unmet medical need. Various strategies have been developed to predict risk of ischemic stroke in patients with atrial fibrillation. CHADS2 <span class="citation">(Gage et al. <a href="#ref-gage2001">2001</a>)</span> was developed as a risk score based on history of congestive heart failure, hypertension, age&gt;=75, diabetes and stroke. CHADS2 was initially derived using Medicare claims data, where it achieved good discrimination (AUC=0.82). However, subsequent external validation studies revealed the CHADS2 had substantially lower predictive accuracy <span class="citation">(Keogh et al. <a href="#ref-keogh2011">2011</a>)</span>. Subsequent stroke risk calculators have been developed and evaluated, including the extension of CHADS2Vasc. The management of atrial fibrillation has evolved substantially over the last decade, for various reasons that include the introduction of novel oral anticoagulants. With these innovations has come a renewed interest in greater precision medicine for stroke prevention.</p>
<p>We will apply the PLP framework to observational healthcare data to address the following patient-level prediction question:</p>
<blockquote>
<p>Amongst patients who are newly diagnosed with Atrial Fibrillation, which patients will go on to have Ischemic Stroke within 1 year?</p>
</blockquote>
<p>We will define ‘patients who are newly diagnosed with Atrial Fibrillation’ as the first condition record of cardiac arrhythmia, which is followed by another cardiac arrhythmia condition record, at least two drug records for a drug used to treat arrhythmias, or a procedure to treat arrhythmias. We will define ‘Ischemic stroke events’ as ischemic stroke condition records during an inpatient or ER visit; successive records with &gt; 180 day gap are considered independent episodes.</p>
</div>
<div id="study-population-definition" class="section level3">
<h3><span class="header-section-number">13.1.2</span> Study population definition</h3>
<p>The final study population in which we will develop our model is often a subset of the Target population, because we will e.g. apply criteria that are dependent on T and O or we want to do sensitivity analyses with subpopulations of T. For this we have to answer the following questions:</p>
<ul>
<li><p><em>What is the minimum amount of observation time we require before the start of the target cohort?</em> This choice could depend on the available patient time in your training data, but also on the time you expect to be available in the data sources you want to apply the model on in the future. The longer the minimum observation time, the more baseline history time is available for each person to use for feature extraction, but the fewer patients will qualify for analysis. Moreover, there could be clinical reasons to choose a short or longer lookback period. For our example, we will use a prior history as lookback period (washout period).</p></li>
<li><p><em>Can patients enter the target cohort multiple times?</em> In the target cohort definition, a person may qualify for the cohort multiple times during different spans of time, for example if they had different episodes of a disease or separate periods of exposure to a medical product. The cohort definition does not necessarily apply a restriction to only let the patients enter once, but in the context of a particular patient-level prediction problem, a user may want to restrict the cohort to the first qualifying episode. In our example, a person could only enter the target cohort once since our criteria was based on first occurrence of atrial fibrillation.</p></li>
<li><p><em>Do we allow persons to enter the cohort if they experienced the outcome before?</em> Do we allow persons to enter the target cohort if they experienced the outcome before qualifying for the target cohort? Depending on the particular patient-level prediction problem, there may be a desire to predict ‘incident’ first occurrence of an outcome, in which case patients who have previously experienced the outcome are not ‘at-risk’ for having a first occurrence and therefore should be excluded from the target cohort. In other circumstances, there may be a desire to predict ‘prevalent’ episodes, whereby patients with prior outcomes can be included in the analysis and the prior outcome itself can be a predictor of future outcomes. For our prediction example, the answer to this question is ‘Yes, allow persons with prior outcomes’ because we know from the CHADS2 score that prior strokes are very predictive of future strokes. If this answer would have been ‘No’ we also have to decide how long we would look back for previous occurrences of the outcome.</p></li>
<li><p><em>How do we define the period in which we will predict our outcome relative to the target cohort start?</em> We actually have to make two decisions to answer that question. First, does the time-at-risk window start at the date of the start of the target cohort or later? Arguments to make it start later could be that you want to avoid outcomes that were entered late in the record that actually occurred before the start of the target cohort or you want to leave a gap where interventions to prevent the outcome could theoretically be implemented. Second, you need to define the time-at-risk by setting the risk window end, as some specification of days offset relative to the target cohort start or end dates. For our problem we will predict in a ‘time-at-risk’ window starting 1 day after the start of the target cohort up to 365 days later (to look for 1-year risk following atrial fibrillation diagnosis).</p></li>
<li><p><em>Do we require a minimum amount of time-at-risk?</em> We have to decide if we want to include patients that did not experience the outcome but did leave the database earlier than the end of our time-at-risk period. These patients may experience the outcome when we do not observe them. For our prediction problem we decide to answer this question with ‘Yes, require a mimimum time-at-risk’ for that reason. Furthermore, we have to decide if this constraint also applies to persons who experienced the outcome or we will include all persons with the outcome irrespective of their total time at risk. For example, if the outcome is death, then persons with the outcome are likely censored before the full time-at-risk period is complete.</p></li>
</ul>
</div>
<div id="model-development-settings" class="section level3">
<h3><span class="header-section-number">13.1.3</span> Model development settings</h3>
<p>To develop the model we have to decide which algorithm(s) we like to train. We see the selection of the best algorithm for a certain prediction problem as an empirical question, i.e. you need to let the data speak for itself and try different approaches to find the best one. There is no algorithm that will work best for all problems (no free lunch). In our framework we therefore aim to implement many algorithms. Furthermore, we made the system modular so you can add your own custom algorithms. This out-of-scope for this chapter but mode details can be found in the AddingCustomAlgorithms vignette (<a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/AddingCustomAlgorithms.pdf" class="uri">https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/AddingCustomAlgorithms.pdf</a>).</p>
<p>Our framework currently contains the following algorithms to choose from:</p>
<table>
<colgroup>
<col width="12%" />
<col width="59%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Description</th>
<th>Hyper-parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regularized Logistic Regression</td>
<td>Lasso logistic regression belongs to the family of generalized linear models, where a linear combination of the variables is learned and finally a logistic function maps the linear combination to a value between 0 and 1. The lasso regularization adds a cost based on model complexity to the objective function when training the model. This cost is the sum of the absolute values of the linear combination of the coefficients. The model automatically performs feature selection by minimizing this cost. We use the Cyclic coordinate descent for logistic, Poisson and survival analysis (Cyclops) package to perform large-scale regularized logistic regression: <a href="https://github.com/OHDSI/Cyclops" class="uri">https://github.com/OHDSI/Cyclops</a></td>
<td>var (starting variance), seed</td>
</tr>
<tr class="even">
<td>Gradient boosting machines</td>
<td>Gradient boosting machines is a boosting ensemble technique and in our framework it combines multiple decision trees. Boosting works by iteratively adding decision trees but adds more weight to the data-points that are misclassified by prior decision trees in the cost function when training the next tree. We use Extreme Gradient Boosting, which is an efficient implementation of the gradient boosting framework implemented in the xgboost R package available from CRAN.</td>
<td>ntree (number of trees), max depth (max levels in tree), min rows (minimum data points in in node), learning rate, seed</td>
</tr>
<tr class="odd">
<td>Random forest</td>
<td>Random forest is a bagging ensemble technique that combines multiple decision trees. The idea behind bagging is to reduce the likelihood of overfitting, by using weak classifiers, but combining multiple diverse weak classifiers into a strong classifier. Random forest accomplishes this by training multiple decision trees but only using a subset of the variables in each tree and the subset of variables differ between trees. Our packages uses the sklearn learn implementation of Random Forest in python.</td>
<td>mtry (number of features in each tree),ntree (number of trees), maxDepth (max levels in tree), minRows (minimum data points in in node),balance (balance class labels), seed</td>
</tr>
<tr class="even">
<td>K-nearest neighbors</td>
<td>K-nearest neighbors (KNN) is an algorithm that uses some metric to find the K closest labelled data-points, given the specified metric, to a new unlabelled data-point. The prediction of the new data-points is then the most prevalent class of the K-nearest labelled data-points. There is a sharing limitation of KNN, as the model requires labelled data to perform the prediction on new data, and it is often not possible to share this data across data sites.We included the BigKnn classifier developed in OHDSI which is a large scale k-nearest neighbor classifier using the Lucene search engine: <a href="https://github.com/OHDSI/BigKnn" class="uri">https://github.com/OHDSI/BigKnn</a></td>
<td>k (number of neighbours),weighted (weight by inverse frequency)</td>
</tr>
<tr class="odd">
<td>Naive Bayes</td>
<td>The Naive Bayes algorithm applies the Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Based on the likelihood the data belongs to a class and the prior distribution of the class, a posterior distribution is obtained.</td>
<td>none</td>
</tr>
<tr class="even">
<td>AdaBoost</td>
<td>AdaBoost is a boosting ensemble technique. Boosting works by iteratively adding classifiers but adds more weight to the data-points that are misclassified by prior classifiers in the cost function when training the next classifier. We use the sklearn “AdaboostClassifier” implementation in Python.</td>
<td>nEstimators (the maximum number of estimators at which boosting is terminated), learningRate (learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learningRate and nEstimators)</td>
</tr>
<tr class="odd">
<td>Decision Tree</td>
<td>A decision tree is a classifier that partitions the variable space using individual tests selected using a greedy approach. It aims to find partitions that have the highest information gain to separate the classes. The decision tree can easily overfit by enabling a large number of partitions (tree depth) and often needs some regularization (e.g., pruning or specifying hyper-parameters that limit the complexity of the model). We use the sklearn “DecisionTreeClassifier” implementation in Python.</td>
<td>maxDepth (the maximum depth of the tree), minSamplesSplit,minSamplesLeaf, minImpuritySplit (threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.), seed,classWeight (“Balance”&quot; or “None”)</td>
</tr>
<tr class="even">
<td>Multilayer Perception</td>
<td>Neural networks contain multiple layers that weight their inputs using a non-linear function. The first layer is the input layer, the last layer is the output layer the between are the hidden layers. Neural networks are generally trained using feed forward back-propagation. This is when you go through the network with a data-point and calculate the error between the true label and predicted label, then go backwards through the network and update the linear function weights based on the error. This can also be performed as a batch, where multiple data-points are fee</td>
<td>size (the number of hidden nodes), alpha (the l2 regularisation), seed</td>
</tr>
<tr class="odd">
<td>Deep Learning</td>
<td>Deep learning such as deep nets, convolutional neural networks or recurrent neural networks are similar to a neural network but have multiple hidden layers that aim to learn latent representations useful for prediction. In the seperate BuildingDeepLearningModels vignette we describe these models and hyper-parameters in more detail</td>
<td></td>
</tr>
</tbody>
</table>
<p>Furthermore, we have to decide on the <strong>covariates</strong> that we will use to train our model. This choice can be driven by domain knowledge of available computational resources. In our example, we like to add the Gender, Age, Conditions, Drugs Groups, and Visit Count. We also have to specify in which time windows we will look and we decide to look in year before and any time prior.</p>
</div>
<div id="model-evaluation" class="section level3">
<h3><span class="header-section-number">13.1.4</span> Model evaluation</h3>
<p>Finally, we have to define how we will train and test our model on our data, i.e. how we perform <strong>internal validation</strong>. For this we have to decide how we divide our dataset in a training and testing dataset and how we randomly assign patients to these two sets. Dependent on the size of the training set we can decide how much data we like to use for training, typically this is a 75%, 25% split. If you have very large datasets you can use more data for training. To randomly assign patients to the training and testing set, there are two commonly used approaches:</p>
<ol style="list-style-type: decimal">
<li>split by person. In this case a random seed is used to assign the patient to either sets.</li>
<li>split by time. In this case a time point is used to split the persons, e.g. 75% of the data is before and 25% is after this date. The advantage of this is that you take into consideration that the health care system has changed over time.</li>
</ol>
<p>For our prediction model we decide to start with a Regularized Logistic Regression and will use the default parameters. We will do a 75%-25% split by person.</p>
</div>
<div id="study-summary" class="section level3">
<h3><span class="header-section-number">13.1.5</span> Study summary</h3>
<p>We now completely defined our study:</p>
<table style="width:99%;">
<colgroup>
<col width="44%" />
<col width="54%" />
</colgroup>
<thead>
<tr class="header">
<th>Definition</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Problem Definition</strong></td>
<td></td>
</tr>
<tr class="even">
<td>Target Cohort (T)</td>
<td>‘Patients who are newly diagnosed with Atrial Fibrillation’ defined as the first condition record of cardiac arrhythmia, which is followed by another cardiac arrhythmia condition record, at least two drug records for a drug used to treat arrhythmias, or a procedure to treat arrhythmias.</td>
</tr>
<tr class="odd">
<td>Outcome Cohort (O)</td>
<td>‘Ischemic stroke events’ defined as ischemic stroke condition records during an inpatient or ER visit; successive records with &gt; 180 day gap are considered independent episodes.</td>
</tr>
<tr class="even">
<td>Time-at-risk (TAR)</td>
<td>1 day till 365 days from cohort start</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Population Definition</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>Washout Period</td>
<td>1095</td>
</tr>
<tr class="even">
<td>Enter the target cohort multiple times?</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Allow prior outcomes?</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Start of time-at-risk</td>
<td>1 day</td>
</tr>
<tr class="odd">
<td>End of time-at-risk</td>
<td>365 days</td>
</tr>
<tr class="even">
<td>Require a minimum amount of time-at-risk?</td>
<td>Yes (364 days)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Model Development</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>Algorithm</td>
<td>Regularized Logistic Regression</td>
</tr>
<tr class="even">
<td>Hyper-parameters</td>
<td>variance = 0.01 (Default)</td>
</tr>
<tr class="odd">
<td>Covariates</td>
<td>ender, Age, Conditions (ever before, &lt;365), Drugs Groups (ever before, &lt;365), and Visit Count</td>
</tr>
<tr class="even">
<td>Data split</td>
<td>75% train, 25% test. Randomly assigned by person</td>
</tr>
</tbody>
</table>
<p>According to the best practices we need to make a protocol that completely specifies how we plan to execute our study. This protocol will be assessed by the governance boards of the participating data sources in your network study. For this a template could be used but we prefer to automate this process as much as possible by adding functionality to automatically generate study protocol from a study specification. We will discuss this in more detail later.</p>
</div>
</div>
<div id="study-implementation" class="section level2">
<h2><span class="header-section-number">13.2</span> Study implementation</h2>
<p>Now we have completely design our study we have to implement the study. This will be done using the <a href="http://github.com/OHDSI/PatientLevelPrediction"><code>PatientLevelPrediction</code></a> package to build patient-level predictive models. The package enables data extraction, model building, and model evaluation using data from databases that are translated into the OMOP CDM. To install the package we like to point you to the InstallationGuide that can be found on the GitHub website (<a href="https://github.com/OHDSI/PatientLevelPrediction/" class="uri">https://github.com/OHDSI/PatientLevelPrediction/</a>).</p>
<p>We first have to generate the target and outcome cohorts and we then need to develop the R code to run against our CDM to execute the full study. These steps will be described in the paragraphs below.</p>
<div id="cohort-instantiation" class="section level3">
<h3><span class="header-section-number">13.2.1</span> Cohort instantiation</h3>
<p>For our study we need to know when a person enters the target and outcome cohorts. This is stored in a table on the server that contains the cohort start date and cohort end date for all subjects for a specific cohort definition. This cohort table has a very simple structure as shown below:</p>
<ul>
<li><code>cohort_definition_id</code>, a unique identifier for distinguishing between different types of cohorts, e.g. cohorts of interest and outcome cohorts.</li>
<li><code>subject_id</code>, a unique identifier corresponding to the <code>person_id</code> in the CDM.</li>
<li><code>cohort_start_date</code>, the date the subject enters the cohort.</li>
<li><code>cohort_end_date</code>, the date the subject leaves the cohort.</li>
</ul>
<p>How do we fill this table according to our cohort definitions? There are two options for this:</p>
<ol style="list-style-type: decimal">
<li><p>use the interactive cohort builder tool in ATLAS (www.github.com/OHDSI/ATLAS) which can be used to create cohorts based on inclusion criteria and will automatically populate this cohort table.</p></li>
<li><p>write your own custom SQL statements to fill the cohort table.</p></li>
</ol>
<p>Both methods are described below for our example prediction problem.</p>
<div id="atlas-cohort-builder" class="section level4">
<h4><span class="header-section-number">13.2.1.1</span> ATLAS cohort builder</h4>
<div class="figure"><span id="fig:atlast"></span>
<img src="images/PatientLevelPrediction/atlast.png" alt="Target Cohort Atrial Fibrillation" width="100%" />
<p class="caption">
Figure 13.4: Target Cohort Atrial Fibrillation
</p>
</div>
<p>ATLAS allows you to define cohorts interactively by specifying cohort entry and cohort exit criteria. Cohort entry criteria involve selecting one or more initial events, which determine the start date for cohort entry, and optionally specifying additional inclusion criteria which filter to the qualifying events. Cohort exit criteria are applied to each cohort entry record to determine the end date when the person’s episode no longer qualifies for the cohort. For the outcome cohort the end date is less relevant. As an example, Figure <a href="PatientLevelPrediction.html#fig:atlast">13.4</a> shows how we created the Atrial Fibrillation cohort and Figure <a href="PatientLevelPrediction.html#fig:atlaso">13.5</a> shows how we created the stroke cohort in ATLAS.</p>
<div class="figure"><span id="fig:atlaso"></span>
<img src="images/PatientLevelPrediction/atlaso.png" alt="Outcome Cohort Stroke" width="100%" />
<p class="caption">
Figure 13.5: Outcome Cohort Stroke
</p>
</div>
<p>The T and O cohorts can be found here:</p>
<ul>
<li>Atrial Fibrillaton (T): <a href="http://www.ohdsi.org/web/atlas/#/cohortdefinition/1769447" class="uri">http://www.ohdsi.org/web/atlas/#/cohortdefinition/1769447</a></li>
<li>Stroke (O) : <a href="http://www.ohdsi.org/web/atlas/#/cohortdefinition/1769448" class="uri">http://www.ohdsi.org/web/atlas/#/cohortdefinition/1769448</a></li>
</ul>
<p>In depth explanation of cohort creation in ATLAS is out of scope of this vignette but can be found on the OHDSI wiki pages (<a href="http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:atlas" class="uri">http://www.ohdsi.org/web/wiki/doku.php?id=documentation:software:atlas</a>).</p>
<p>Note that when a cohort is created in ATLAS the cohortid is needed to extract the data in R. The cohortid can be found at the top of the ATLAS screen.</p>
</div>
<div id="custom-cohorts" class="section level4">
<h4><span class="header-section-number">13.2.1.2</span> Custom cohorts</h4>
<p>It is also possible to create cohorts without the use of ATLAS. Using custom cohort code (SQL) you can make more advanced cohorts if needed.</p>
<p>For our example study, we need to create at table to hold the cohort data and we need to create SQL code to instantiate this table for both the AF and Stroke cohorts. Therefore, we create a file called <em>AfStrokeCohorts.sql</em> with the following contents:</p>
<div class="sourceCode"><pre class="sourceCode sql"><code class="sourceCode sql"><span class="co">/***********************************</span>
<span class="co">File AfStrokeCohorts.sql</span>
<span class="co">***********************************/</span>
<span class="co">/*</span>
<span class="co">  Create a table to store the persons in the T and C cohort</span>
<span class="co">*/</span>

<span class="kw">IF</span> OBJECT_ID(<span class="st">&#39;@resultsDatabaseSchema.PLPAFibStrokeCohort&#39;</span>, <span class="st">&#39;U&#39;</span>) <span class="kw">IS</span> <span class="kw">NOT</span> <span class="kw">NULL</span>
  <span class="kw">DROP</span> <span class="kw">TABLE</span> @resultsDatabaseSchema.PLPAFibStrokeCohort;

<span class="kw">CREATE</span> <span class="kw">TABLE</span> @resultsDatabaseSchema.PLPAFibStrokeCohort
(
  cohort_definition_id <span class="dt">INT</span>,
  subject_id BIGINT,
  cohort_start_date <span class="dt">DATE</span>,
  cohort_end_date <span class="dt">DATE</span>
);


<span class="co">/*</span>
<span class="co">  T cohort:  [PatientLevelPrediction vignette]:  T : patients who are newly</span>
<span class="co">             diagnosed with Atrial fibrillation</span>
<span class="co">  - persons with a condition occurrence record of &#39;Atrial fibrillation&#39; or</span>
<span class="co">    any descendants, indexed at the first diagnosis</span>
<span class="co">  - who have &gt;1095 days of prior observation before their first diagnosis</span>
<span class="co">  - and have no warfarin exposure any time prior to first AFib diagnosis</span>
<span class="co">*/</span>
<span class="kw">INSERT</span> <span class="kw">INTO</span> @resultsDatabaseSchema.AFibStrokeCohort (cohort_definition_id,
                                                     subject_id,
                                                     cohort_start_date,
                                                     cohort_end_date)
<span class="kw">SELECT</span> <span class="dv">1</span> <span class="kw">AS</span> cohort_definition_id,
  AFib.person_id <span class="kw">AS</span> subject_id,
  AFib.condition_start_date <span class="kw">AS</span> cohort_start_date,
  observation_period.observation_period_end_date <span class="kw">AS</span> cohort_end_date
<span class="kw">FROM</span>
(
  <span class="kw">SELECT</span> person_id, <span class="fu">min</span>(condition_start_date) <span class="kw">as</span> condition_start_date
  <span class="kw">FROM</span> @cdmDatabaseSchema.condition_occurrence
  <span class="kw">WHERE</span> condition_concept_id <span class="kw">IN</span> (<span class="kw">SELECT</span> descendant_concept_id <span class="kw">FROM</span>
        @cdmDatabaseSchema.concept_ancestor <span class="kw">WHERE</span> ancestor_concept_id <span class="kw">IN</span>
        (<span class="dv">313217</span> <span class="co">/*atrial fibrillation*/</span>))
  <span class="kw">GROUP</span> <span class="kw">BY</span> person_id
) AFib
<span class="kw">INNER</span> <span class="kw">JOIN</span> @cdmDatabaseSchema.observation_period
  <span class="kw">ON</span> AFib.person_id = observation_period.person_id
  <span class="kw">AND</span> AFib.condition_start_date &gt;= dateadd(dd,<span class="dv">1095</span>,
                                   observation_period.observation_period_start_date)
  <span class="kw">AND</span> AFib.condition_start_date &lt;= observation_period.observation_period_end_date
<span class="kw">LEFT</span> <span class="kw">JOIN</span>
(
  <span class="kw">SELECT</span> person_id, <span class="fu">min</span>(drug_exposure_start_date) <span class="kw">as</span> drug_exposure_start_date
  <span class="kw">FROM</span> @cdmDatabaseSchema.drug_exposure
  <span class="kw">WHERE</span> drug_concept_id <span class="kw">IN</span> (<span class="kw">SELECT</span> descendant_concept_id <span class="kw">FROM</span>
       @cdmDatabaseSchema.concept_ancestor <span class="kw">WHERE</span> ancestor_concept_id <span class="kw">IN</span>
       (<span class="dv">1310149</span> <span class="co">/*warfarin*/</span>))
  <span class="kw">GROUP</span> <span class="kw">BY</span> person_id
) warfarin
  <span class="kw">ON</span> Afib.person_id = warfarin.person_id
  <span class="kw">AND</span> Afib.condition_start_date &gt; warfarin.drug_exposure_start_date
<span class="kw">WHERE</span> warfarin.person_id <span class="kw">IS</span> <span class="kw">NULL</span>
;

<span class="co">/*</span>
<span class="co">  C cohort:  [PatientLevelPrediction vignette]:  O: Ischemic stroke events</span>
<span class="co">  - inpatient visits that include a condition occurrence record for</span>
<span class="co">    &#39;cerebral infarction&#39; and descendants, &#39;cerebral thrombosis&#39;,</span>
<span class="co">    &#39;cerebral embolism&#39;, &#39;cerebral artery occlusion&#39;</span>
<span class="co">*/</span>
<span class="kw">INSERT</span> <span class="kw">INTO</span> @resultsDatabaseSchema.AFibStrokeCohort (cohort_definition_id,
                                                     subject_id,
                                                     cohort_start_date,
                                                     cohort_end_date)
<span class="kw">SELECT</span> <span class="dv">2</span> <span class="kw">AS</span> cohort_definition_id,
  visit_occurrence.person_id <span class="kw">AS</span> subject_id,
  visit_occurrence.visit_start_date <span class="kw">AS</span> cohort_start_date,
  visit_occurrence.visit_end_date <span class="kw">AS</span> cohort_end_date
<span class="kw">FROM</span>
(
  <span class="kw">SELECT</span> person_id, condition_start_date
  <span class="kw">FROM</span> @cdmDatabaseSchema.condition_occurrence
  <span class="kw">WHERE</span> condition_concept_id <span class="kw">IN</span> (<span class="kw">SELECT</span> <span class="kw">DISTINCT</span> descendant_concept_id <span class="kw">FROM</span>
  @cdmDatabaseSchema.concept_ancestor <span class="kw">WHERE</span> ancestor_concept_id <span class="kw">IN</span>
  (<span class="dv">443454</span> <span class="co">/*cerebral infarction*/</span>) <span class="kw">OR</span> descendant_concept_id <span class="kw">IN</span>
  (<span class="dv">441874</span> <span class="co">/*cerebral thrombosis*/</span>, <span class="dv">375557</span> <span class="co">/*cerebral embolism*/</span>,
   <span class="dv">372924</span> <span class="co">/*cerebral artery occlusion*/</span>))
) stroke
<span class="kw">INNER</span> <span class="kw">JOIN</span> @cdmDatabaseSchema.visit_occurrence
<span class="kw">ON</span> stroke.person_id = visit_occurrence.person_id
<span class="kw">AND</span> stroke.condition_start_date &gt;= visit_occurrence.visit_start_date
<span class="kw">AND</span> stroke.condition_start_date &lt;= visit_occurrence.visit_end_date
<span class="kw">AND</span> visit_occurrence.visit_concept_id <span class="kw">IN</span> (<span class="dv">9201</span>, <span class="dv">262</span> <span class="co">/*&#39;Inpatient Visit&#39;  or</span>
<span class="co">    &#39;Emergency Room and Inpatient Visit&#39;*/</span>)
<span class="kw">GROUP</span> <span class="kw">BY</span> visit_occurrence.person_id, visit_occurrence.visit_start_date,
         visit_occurrence.visit_end_date
;</code></pre></div>
<p>This is parameterized SQL which can be used by the <a href="http://github.com/OHDSI/SqlRender"><code>SqlRender</code></a> package. We use parameterized SQL so we do not have to pre-specify the names of the CDM and result schemas. That way, if we want to run the SQL on a different schema, we only need to change the parameter values; we do not have to change the SQL code. By also making use of translation functionality in <code>SqlRender</code>, we can make sure the SQL code can be run in many different environments.</p>
<p>To execute this SQL against our CDM we first need to tell R how to connect to the server. <a href="http://github.com/OHDSI/PatientLevelPrediction"><code>PatientLevelPrediction</code></a> uses the <a href="http://github.com/ohdsi/DatabaseConnector"><code>DatabaseConnector</code></a> package, which provides a function called <code>createConnectionDetails</code>. Type <code>?createConnectionDetails</code> for the specific settings required for the various database management systems (DBMS). For example, one might connect to a PostgreSQL database using this code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">connectionDetails &lt;-<span class="st"> </span><span class="kw">createConnectionDetails</span>(<span class="dt">dbms =</span> <span class="st">&quot;postgresql&quot;</span>,
                                             <span class="dt">server =</span> <span class="st">&quot;localhost/ohdsi&quot;</span>,
                                             <span class="dt">user =</span> <span class="st">&quot;joe&quot;</span>,
                                             <span class="dt">password =</span> <span class="st">&quot;supersecret&quot;</span>)

cdmDatabaseSchema &lt;-<span class="st"> &quot;my_cdm_data&quot;</span>
cohortsDatabaseSchema &lt;-<span class="st"> &quot;my_results&quot;</span>
cdmVersion &lt;-<span class="st"> &quot;5&quot;</span></code></pre></div>
<p>The last three lines define the <code>cdmDatabaseSchema</code> and <code>cohortsDatabaseSchema</code> variables, as well as the CDM version. We will use these later to tell R where the data in CDM format live, where we want to create the cohorts of interest, and what version CDM is used. Note that for Microsoft SQL Server, databaseschemas need to specify both the database and the schema, so for example <code>cdmDatabaseSchema &lt;- &quot;my_cdm_data.dbo&quot;</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(SqlRender)
sql &lt;-<span class="st"> </span><span class="kw">readSql</span>(<span class="st">&quot;AfStrokeCohorts.sql&quot;</span>)
sql &lt;-<span class="st"> </span><span class="kw">renderSql</span>(sql,
<span class="dt">cdmDatabaseSchema =</span> cdmDatabaseSchema,
<span class="dt">cohortsDatabaseSchema =</span> cohortsDatabaseSchema,
<span class="dt">post_time =</span> <span class="dv">30</span>,
<span class="dt">pre_time =</span> <span class="dv">365</span>)<span class="op">$</span>sql
sql &lt;-<span class="st"> </span><span class="kw">translateSql</span>(sql, <span class="dt">targetDialect =</span> connectionDetails<span class="op">$</span>dbms)<span class="op">$</span>sql

connection &lt;-<span class="st"> </span><span class="kw">connect</span>(connectionDetails)
<span class="kw">executeSql</span>(connection, sql)</code></pre></div>
<p>In this code, we first read the SQL from the file into memory. In the next line, we replace four parameter names with the actual values. We then translate the SQL into the dialect appropriate for the DBMS we already specified in the <code>connectionDetails</code>. Next, we connect to the server, and submit the rendered and translated SQL.</p>
<p>If all went well, we now have a table with the events of interest. We can see how many events per type:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sql &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;SELECT cohort_definition_id, COUNT(*) AS count&quot;</span>,
<span class="st">&quot;FROM @cohortsDatabaseSchema.AFibStrokeCohort&quot;</span>,
<span class="st">&quot;GROUP BY cohort_definition_id&quot;</span>)
sql &lt;-<span class="st"> </span><span class="kw">renderSql</span>(sql, <span class="dt">cohortsDatabaseSchema =</span> cohortsDatabaseSchema)<span class="op">$</span>sql
sql &lt;-<span class="st"> </span><span class="kw">translateSql</span>(sql, <span class="dt">targetDialect =</span> connectionDetails<span class="op">$</span>dbms)<span class="op">$</span>sql

<span class="kw">querySql</span>(connection, sql)</code></pre></div>
<pre><code>##   cohort_definition_id  count
## 1                    1 527616
## 2                    2 221555</code></pre>
</div>
</div>
<div id="study-script-creation" class="section level3">
<h3><span class="header-section-number">13.2.2</span> Study script creation</h3>
<p>In this section we assume that our cohorts have been created either by using ATLAS or a custom SQL script. We will first explain how to create an R script yourself that will execute our study as we have defined earlier.</p>
<div id="data-extraction" class="section level4">
<h4><span class="header-section-number">13.2.2.1</span> Data extraction</h4>
<p>Now we can tell <a href="http://github.com/OHDSI/PatientLevelPrediction"><code>PatientLevelPrediction</code></a> to extract all necessary data for our analysis. This is done using the FeatureExtractionPackage package (<a href="https://github.com/OHDSI/FeatureExtration" class="uri">https://github.com/OHDSI/FeatureExtration</a>). In short the FeatureExtractionPackage allows you to specify which features (covariates) need to be extracted, e.g. all conditions and drug exposures, more information can be found in chapter X. It also supports the creation of custom covariates. For more detailed information on the FeatureExtraction package see its vignettes. For our example study we decided to use these settings:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covariateSettings &lt;-<span class="st"> </span><span class="kw">createCovariateSettings</span>(<span class="dt">useDemographicsGender =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">useDemographicsAge =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">useConditionGroupEraLongTerm =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">useConditionGroupEraAnyTimePrior =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">useDrugGroupEraLongTerm =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">useDrugGroupEraAnyTimePrior =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">useVisitConceptCountLongTerm =</span> <span class="ot">TRUE</span>,
                                             <span class="dt">longTermStartDays =</span> <span class="op">-</span><span class="dv">365</span>,
                                             <span class="dt">endDays =</span> <span class="op">-</span><span class="dv">1</span>)</code></pre></div>
<p>The final step for extracting the data is to run the <code>getPlpData</code> function and input the connection details, the database schema where the cohorts are stored, the cohort definition ids for the cohort and outcome, and the washoutPeriod which is the minimum number of days prior to cohort index date that the person must have been observed to be included into the data, and finally input the previously constructed covariate settings.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plpData &lt;-<span class="st"> </span><span class="kw">getPlpData</span>(<span class="dt">connectionDetails =</span> connectionDetails,
                      <span class="dt">cdmDatabaseSchema =</span> cdmDatabaseSchema,
                      <span class="dt">cohortDatabaseSchema =</span> resultsDatabaseSchema,
                      <span class="dt">cohortTable =</span> <span class="st">&#39;AFibStrokeCohort&#39;</span>,
                      <span class="dt">cohortId =</span> <span class="dv">1</span>,
                      <span class="dt">covariateSettings =</span> covariateSettings,
                      <span class="dt">outcomeDatabaseSchema =</span> resultsDatabaseSchema,
                      <span class="dt">outcomeTable =</span> <span class="st">&#39;AFibStrokeCohort&#39;</span>,
                      <span class="dt">outcomeIds =</span> <span class="dv">2</span>,
                      <span class="dt">sampleSize =</span> <span class="dv">10000</span>
)</code></pre></div>
<p>Note that if the cohorts are created in ATLAS its corresponding cohort database schema needs to be selected. There are many additional parameters for the <code>getPlpData</code> function which are all documented in the <a href="http://github.com/OHDSI/PatientLevelPrediction"><code>PatientLevelPrediction</code></a> manual. The resulting <code>plpData</code> object uses the package <code>ff</code> to store information in a way that ensures R does not run out of memory, even when the data are large.</p>
<p>Creating the <code>plpData</code> object can take considerable computing time, and it is probably a good idea to save it for future sessions. Because <code>plpData</code> uses <code>ff</code>, we cannot use R’s regular save function. Instead, we’ll have to use the <code>savePlpData()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">savePlpData</span>(plpData, <span class="st">&quot;stroke_in_af_data&quot;</span>)</code></pre></div>
<p>We can use the <code>loadPlpData()</code> function to load the data in a future session.</p>
</div>
<div id="additional-inclusion-criteria" class="section level4">
<h4><span class="header-section-number">13.2.2.2</span> Additional inclusion criteria</h4>
<p>To completely define the prediction problem the final study population is obtained by applying additional constraints on the two earlier defined cohorts, e.g., a minumim time at risk can be enforced (<code>requireTimeAtRisk, minTimeAtRisk</code>) and we can specify if this also applies to patients with the outcome (<code>includeAllOutcomes</code>). Here we also specify the start and end of the risk window relative to target cohort start. For example, if we like the risk window to start 30 days after the at-risk cohort start and end a year later we can set <code>riskWindowStart = 30</code> and <code>riskWindowEnd = 365</code>. In some cases the risk window needs to start at the cohort end date. This can be achieved by setting <code>addExposureToStart = TRUE</code> which adds the cohort (exposure) time to the start date.</p>
<p>In Appendix 1, we demonstrate the effect of these settings on the subset of the persons in the target cohort that end up in the final study population.</p>
<p>In the example below all the settings we defined for our study are imposed:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">population &lt;-<span class="st"> </span><span class="kw">createStudyPopulation</span>(<span class="dt">plpData =</span> plpData,
                                    <span class="dt">outcomeId =</span> <span class="dv">2</span>,
                                    <span class="dt">washoutPeriod =</span> <span class="dv">1095</span>,
                                    <span class="dt">firstExposureOnly =</span> <span class="ot">FALSE</span>,
                                    <span class="dt">removeSubjectsWithPriorOutcome =</span> <span class="ot">FALSE</span>,
                                    <span class="dt">priorOutcomeLookback =</span> <span class="dv">1</span>,
                                    <span class="dt">riskWindowStart =</span> <span class="dv">1</span>,
                                    <span class="dt">riskWindowEnd =</span> <span class="dv">365</span>,
                                    <span class="dt">addExposureDaysToStart =</span> <span class="ot">FALSE</span>,
                                    <span class="dt">addExposureDaysToEnd =</span> <span class="ot">FALSE</span>,
                                    <span class="dt">minTimeAtRisk =</span> <span class="dv">364</span>,
                                    <span class="dt">requireTimeAtRisk =</span> <span class="ot">TRUE</span>,
                                    <span class="dt">includeAllOutcomes =</span> <span class="ot">TRUE</span>,
                                    <span class="dt">verbosity =</span> <span class="st">&quot;DEBUG&quot;</span>
                                    )</code></pre></div>
</div>
<div id="model-development" class="section level4">
<h4><span class="header-section-number">13.2.2.3</span> Model Development</h4>
<p>In the set function of an algorithm the user can specify a list of eligible values for each hyper-parameter. All possible combinations of the hyper-parameters are included in a so-called grid search using cross-validation on the training set. If a user does not specify any value then the default value is used instead.</p>
<p>For example, if we use the following settings for the gradientBoostingMachine: ntrees=c(100,200), maxDepth=4 the grid search will apply the gradient boosting machine algorithm with ntrees=100 and maxDepth=4 plus the default settings for other hyper-parameters and ntrees=200 and maxDepth=4 plus the default settings for other hyper-parameters. The hyper-parameters that lead to the bestcross-validation performance will then be chosen for the final model. For our problem we choose to build a logistic regression model with the default hyper-parameters</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrModel &lt;-<span class="st"> </span><span class="kw">setLassoLogisticRegression</span>()</code></pre></div>
<p>The <code>runPlP</code> function uses the population, <code>plpData</code>, and model settings to train and evaluate the model. We can use the testSplit (person/time) and testFraction parameters to split the data in a 75%-25% split and run the patient-level prediction pipeline:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrResults &lt;-<span class="st"> </span><span class="kw">runPlp</span>(population, plpData, <span class="dt">modelSettings =</span> lrModel, <span class="dt">testSplit=</span><span class="st">&#39;person&#39;</span>,
                    <span class="dt">testFraction=</span><span class="fl">0.25</span>, <span class="dt">nfold=</span><span class="dv">2</span>, <span class="dt">splitSeed =</span> <span class="dv">1234</span>)</code></pre></div>
<p>Under the hood the package will now use the <a href="http://github.com/OHDSI/Cyclops"><code>Cyclops</code></a> package fit a large-scale regularized regression using 75% of the data and will evaluate the model on the remaining 25%. A results data structure is returned containing information about the model, its performance etc.</p>
<p>In the <code>runPlp</code> function there are several parameters to save the <code>plpData</code>, <code>plpResults</code>, <code>plpPlots</code>, <code>evaluation</code>, etc. objects which are all set to <code>true</code> by default. However, there is also some functionality to this manually.</p>
<p>You can save the model using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">savePlpModel</span>(lrResults<span class="op">$</span>model, <span class="dt">dirPath =</span> <span class="kw">file.path</span>(<span class="kw">getwd</span>(), <span class="st">&quot;model&quot;</span>))</code></pre></div>
<p>You can load the model using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plpModel &lt;-<span class="st"> </span><span class="kw">loadPlpModel</span>(<span class="kw">getwd</span>(),<span class="st">&#39;model&#39;</span>)</code></pre></div>
<p>You can also save the full results structure using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">savePlpResult</span>(lrResults, <span class="dt">location =</span> <span class="kw">file.path</span>(<span class="kw">getwd</span>(),<span class="st">&#39;lr&#39;</span>))</code></pre></div>
<p>To load the full results structure use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrResults &lt;-<span class="st"> </span><span class="kw">loadPlpResult</span>(<span class="kw">file.path</span>(<span class="kw">getwd</span>(),<span class="st">&#39;lr&#39;</span>))</code></pre></div>

</div>
</div>
<div id="study-package-creation" class="section level3">
<h3><span class="header-section-number">13.2.3</span> Study package creation</h3>
<p>The script we created manually above can also be automatically created using a powerful feature in ATLAS. By creating a new prediction study (left menu) you can select the Target and Outcome as created in ATLAS, set all the study parameters, and then you can download a R package that you can use to execute your study. What is really powerful is that you can add multiple Ts, Os, covariate settings etc. The package will then run all the combinations of automatically as separate analyses. The screenshots below explain this process.</p>
<ol style="list-style-type: decimal">
<li>Create a new prediction study and select your target and outcome cohorts.</li>
</ol>
<p><img src="images/PatientLevelPrediction/atlasplp1.png" width="100%" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Specify one or more analysis settings</li>
</ol>
<p><img src="images/PatientLevelPrediction/atlasplp2.png" width="100%" /></p>

<ol start="3" style="list-style-type: decimal">
<li>Specify the trainings settigns.</li>
</ol>
<p><img src="images/PatientLevelPrediction/atlasplp3.png" width="100%" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Specify the execution settings.</li>
</ol>
<p><img src="images/PatientLevelPrediction/atlasplp4.png" width="100%" /></p>

<p>ATLAS can build a R package for you that will execute the full study against you CDM. Below the steps are explained how to do this in ATLAS.</p>
<ol style="list-style-type: decimal">
<li>Under utilities you can find download. Click on the button to review the full study specification.</li>
</ol>
<p><img src="images/PatientLevelPrediction/atlasdownload1.png" width="100%" /></p>
<ol start="2" style="list-style-type: decimal">
<li>You now have to review that you indeed want to run all these analyses (cartesian product of all the settings for each T and O combination.</li>
</ol>
<p><img src="images/PatientLevelPrediction/atlasdownload2.png" width="100%" /></p>
<ol start="3" style="list-style-type: decimal">
<li><p>If you agree, you give the package a name, and download the package as a zipfile.</p></li>
<li><p>By opening the R package in R studio and building the package you can run the study using the <code>execute</code> function. Theres is also an example CodeToRun.R script available in the extras folder of the package with extra instructions.</p></li>
</ol>
</div>
</div>
<div id="internal-validation" class="section level2">
<h2><span class="header-section-number">13.3</span> Internal validation</h2>
<p>Once we execute the study, the runPlp() function returns the trained model and the evaluation of the model on the train/test sets.</p>
<p>You can interactively view the results by running: <code>viewPlp(runPlp=lrResults)</code>. This will generate a Shiny App in your browser in which you can view all performance measures created by the framework as shown in the figure below.</p>
<p><img src="images/PatientLevelPrediction/shinysummary.png" width="100%" /></p>
<p>Furthermore, many interactive plots are available in the Shiny App, for example the ROC curve in which you can move over the plot to see the threshold and the corresponding sensitivity and specificity values.</p>
<p><img src="images/PatientLevelPrediction/shinyroc.png" width="100%" /></p>
<p>To generate and save all the evaluation plots to a folder run the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotPlp</span>(lrResults, <span class="dt">dirPath=</span><span class="kw">getwd</span>())</code></pre></div>
<p>The plots are described in more detail in the next sections.</p>

<div id="discrimination" class="section level3">
<h3><span class="header-section-number">13.3.1</span> Discrimination</h3>
<p>The Receiver Operating Characteristics (ROC) plot shows the sensitivity against 1-specificity on the test set. The plot illustrates how well the model is able to discriminate between the people with the outcome and those without. The dashed diagonal line is the performance of a model that randomly assigns predictions. The higher the area under the ROC plot the better the discrimination of the model. The plot is created by changing the probability threshold to assign the positive class.</p>
<p><img src="images/PatientLevelPrediction/sparseROC.png" width="100%" /></p>

</div>
<div id="calibration" class="section level3">
<h3><span class="header-section-number">13.3.2</span> Calibration</h3>
<p>The calibration plot shows how close the predicted risk is to the observed risk. The diagonal dashed line thus indicates a perfectly calibrated model. The ten (or fewer) dots represent the mean predicted values for each quantile plotted against the observed fraction of people in that quantile who had the outcome (observed fraction). The straight black line is the linear regression using these 10 plotted quantile mean predicted vs observed fraction points. The straight vertical lines represented the 95% lower and upper confidence intervals of the slope of the fitted line.</p>
<p><img src="images/PatientLevelPrediction/sparseCalibration.png" width="100%" /></p>

</div>
<div id="smooth-calibration" class="section level3">
<h3><span class="header-section-number">13.3.3</span> Smooth Calibration</h3>
<p>Similar to the traditional calibration shown above the Smooth Calibration plot shows the relationship between predicted and observed risk. the major difference is that the smooth fit allows for a more fine grained examination of this. Whereas the traditional plot will be heavily influenced by the areas with the highest density of data the smooth plot will provide the same information for this region as well as a more accurate interpretation of areas with lower density. the plot also contains information on the distribution of the outcomes relative to predicted risk.</p>
<p>However, the increased information gain comes at a computational cost. It is recommended to use the traditional plot for examination and then to produce the smooth plot for final versions. To create the smooth calibarion plot you have to run the follow command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotSmoothCalibration</span>(lrResults)</code></pre></div>
<p>See the help function for more information, on how to set the smoothing method etc.</p>
<p>The example below is from another study that better demonstrates the impact of using a smooth calibration plot. The default line fit would not highlight the miss-calibration at the lower predicted probability levels that well.</p>
<p><img src="images/PatientLevelPrediction/smoothCalibration.jpeg" width="100%" /></p>

</div>
<div id="preference-distribution" class="section level3">
<h3><span class="header-section-number">13.3.4</span> Preference distribution</h3>
<p>The preference distribution plots are the preference score distributions corresponding to i) people in the test set with the outcome (red) and ii) people in the test set without the outcome (blue).</p>
<p><img src="images/PatientLevelPrediction/preferencePDF.png" width="100%" /></p>

</div>
<div id="predicted-probability-distribution" class="section level3">
<h3><span class="header-section-number">13.3.5</span> Predicted probability distribution</h3>
<p>The prediction distribution box plots are for the predicted risks of the people in the test set with the outcome (class 1: blue) and without the outcome (class 0: red).</p>
<p>The box plots in the Figure show that the predicted probability of the outcome is indeed higher for those with the outcome but there is also overlap between the two distribution which lead to an imperfect discrimination.</p>
<p><img src="images/PatientLevelPrediction/predictionDistribution.png" width="100%" /></p>

</div>
<div id="test-train-similarity" class="section level3">
<h3><span class="header-section-number">13.3.6</span> Test-Train similarity</h3>
<p>The test-train similarity is assessed by plotting the mean covariate values in the train set against those in the test set for people with and without the outcome.</p>
<p>The results for our example of look very promising since the mean values of the covariates are on the diagonal.</p>
<p><img src="images/PatientLevelPrediction/generalizability.png" width="100%" /></p>

</div>
<div id="variable-scatter-plot" class="section level3">
<h3><span class="header-section-number">13.3.7</span> Variable scatter plot</h3>
<p>The variable scatter plot shows the mean covariate value for the people with the outcome against the mean covariate value for the people without the outcome. The color of the dots corresponds to the inclusion (green) or exclusion in the model (blue), respectively. It is highly recommended to use the Shiny App since this allows you to hoover over a covariate to show more details (name, value etc).</p>
<p>The plot shows that the mean of most of the covariates is higher for subjects with the outcome compared to those without.</p>
<p><img src="images/PatientLevelPrediction/variableScatterplot.png" width="100%" /></p>

</div>
<div id="precision-recall" class="section level3">
<h3><span class="header-section-number">13.3.8</span> Precision recall</h3>
<p>Precision (P) is defined as the number of true positives (Tp) over the number of true positives plus the number of false positives (Fp).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">P &lt;-<span class="st"> </span>Tp<span class="op">/</span>(Tp<span class="op">+</span>Fp)</code></pre></div>
<p>Recall (R) is defined as the number of true positives (Tp) over the number of true positives plus the number of false negatives (Fn).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">R &lt;-<span class="st"> </span>Tp<span class="op">/</span>(Tp <span class="op">+</span><span class="st"> </span>Fn)</code></pre></div>
<p>These quantities are also related to the (F1) score, which is defined as the harmonic mean of precision and recall.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">F1 &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>P<span class="op">*</span>R<span class="op">/</span>(P<span class="op">+</span>R)</code></pre></div>
<p>Note that the precision can either decrease or increase if the threshold is lowered. Lowering the threshold of a classifier may increase the denominator, by increasing the number of results returned. If the threshold was previously set too high, the new results may all be true positives, which will increase precision. If the previous threshold was about right or too low, further lowering the threshold will introduce false positives, decreasing precision.</p>
<p>For Recall the denominator does not depend on the classifier threshold (Tp+Fn is a constant). This means that lowering the classifier threshold may increase recall, by increasing the number of true positive results. It is also possible that lowering the threshold may leave recall unchanged, while the precision fluctuates.</p>
<p><img src="images/PatientLevelPrediction/precisionRecall.png" width="100%" /></p>

</div>
<div id="demographic-summary" class="section level3">
<h3><span class="header-section-number">13.3.9</span> Demographic summary</h3>
<p>This plot shows for females and males the expected and observed risk in different age groups together with a confidence area.</p>
<p>The results show that our model is well calibrated across gender and age groups.</p>
<p><img src="images/PatientLevelPrediction/demographicSummary.png" width="100%" /></p>

</div>
</div>
<div id="external-validation" class="section level2">
<h2><span class="header-section-number">13.4</span> External validation</h2>
<p>We recommend to always perform external validation, i.e. apply the final model on as much new datasets as feasible and evaluate its performance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the trained model</span>
plpModel &lt;-<span class="st"> </span><span class="kw">loadPlpModel</span>(<span class="kw">getwd</span>(),<span class="st">&#39;model&#39;</span>)

<span class="co">#load the new plpData and create the population</span>
plpData &lt;-<span class="st"> </span><span class="kw">loadPlpData</span>(<span class="kw">getwd</span>(),<span class="st">&#39;data&#39;</span>)
population &lt;-<span class="st"> </span><span class="kw">createStudyPopulation</span>(plpData,
<span class="dt">outcomeId =</span> <span class="dv">2</span>,
<span class="dt">includeAllOutcomes =</span> <span class="ot">TRUE</span>,
<span class="dt">firstExposureOnly =</span> <span class="ot">TRUE</span>,
<span class="dt">washoutPeriod =</span> <span class="dv">365</span>,
<span class="dt">removeSubjectsWithPriorOutcome =</span> <span class="ot">TRUE</span>,
<span class="dt">priorOutcomeLookback =</span> <span class="dv">365</span>,
<span class="dt">riskWindowStart =</span> <span class="dv">1</span>,
<span class="dt">requireTimeAtRisk =</span> <span class="ot">FALSE</span>,
<span class="dt">riskWindowEnd =</span> <span class="dv">365</span>
)

<span class="co"># apply the trained model on the new data</span>
validationResults &lt;-<span class="st"> </span><span class="kw">applyModel</span>(population,plpData,plpModel)</code></pre></div>
<p>To make things easier we also provide a function for performing external validation of a model across one or multiple datasets:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the trained model</span>
plpResult &lt;-<span class="st"> </span><span class="kw">loadPlpResult</span>(<span class="kw">getwd</span>(),<span class="st">&#39;plpResult&#39;</span>)

connectionDetails &lt;-<span class="st"> </span><span class="kw">createConnectionDetails</span>(<span class="dt">dbms =</span> <span class="st">&quot;postgresql&quot;</span>,
                                             <span class="dt">server =</span> <span class="st">&quot;localhost/ohdsi&quot;</span>,
                                             <span class="dt">user =</span> <span class="st">&quot;joe&quot;</span>,
                                             <span class="dt">password =</span> <span class="st">&quot;supersecret&quot;</span>)

validation &lt;-<span class="st"> </span><span class="kw">externalValidatePlp</span>(<span class="dt">plpResult =</span> plpResult,
                                  <span class="dt">connectionDetails =</span> connectionDetails,
                                  <span class="dt">validationSchemaTarget =</span> <span class="st">&#39;new_cohort_schema&#39;</span>,
                                  <span class="dt">validationSchemaOutcome =</span> <span class="st">&#39;new_cohort_schema&#39;</span>,
                                  <span class="dt">validationSchemaCdm =</span> <span class="st">&#39;new_cdm_schema&#39;</span>,
                                  <span class="dt">validationTableTarget =</span> <span class="st">&#39;cohort_table&#39;</span>,
                                  <span class="dt">validationTableOutcome =</span> <span class="st">&#39;cohort_table&#39;</span>,
                                  <span class="dt">validationIdTarget =</span> <span class="st">&#39;cohort_id&#39;</span>,
                                  <span class="dt">validationIdOutcome =</span> <span class="st">&#39;outcome_id&#39;</span>,
                                  <span class="dt">keepPrediction =</span> T
                                  )</code></pre></div>
<p>This will extract the new plpData from the specified schemas and cohort tables. It will then apply the same population settings and the trained plp model. Finally, it will evaluate the performance and return the standard output as <code>validation$performance</code> and if keepPrediction is TRUE then it will also return the prediction on the population as <code>validation$prediction</code>. They can be inserted into the shiny app for viewing the model and validation by running: <code>viewPlp(runPlp=plpResult, validatePlp=validation )</code>.</p>
<p>If you want to validate on multiple databases available you can insert the new schemas and cohort tables as a list:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the trained model</span>
plpResult &lt;-<span class="st"> </span><span class="kw">loadPlpResult</span>(<span class="kw">getwd</span>(),<span class="st">&#39;plpResult&#39;</span>)

connectionDetails &lt;-<span class="st"> </span><span class="kw">createConnectionDetails</span>(<span class="dt">dbms =</span> <span class="st">&quot;postgresql&quot;</span>,
                                             <span class="dt">server =</span> <span class="st">&quot;localhost/ohdsi&quot;</span>,
                                             <span class="dt">user =</span> <span class="st">&quot;joe&quot;</span>,
                                             <span class="dt">password =</span> <span class="st">&quot;supersecret&quot;</span>)

validation &lt;-<span class="st"> </span><span class="kw">externalValidatePlp</span>(<span class="dt">plpResult =</span> plpResult,
                                  <span class="dt">connectionDetails =</span> connectionDetails,
                                  <span class="dt">validationSchemaTarget =</span> <span class="kw">list</span>(<span class="st">&#39;new_cohort_schema1&#39;</span>,
                                                                <span class="st">&#39;new_cohort_schema2&#39;</span>),
                                  <span class="dt">validationSchemaOutcome =</span> <span class="kw">list</span>(<span class="st">&#39;new_cohort_schema1&#39;</span>,
                                                                 <span class="st">&#39;new_cohort_schema2&#39;</span>),
                                  <span class="dt">validationSchemaCdm =</span> <span class="kw">list</span>(<span class="st">&#39;new_cdm_schema1&#39;</span>,
                                                             <span class="st">&#39;new_cdm_schema2&#39;</span>),
                                  <span class="dt">validationTableTarget =</span> <span class="kw">list</span>(<span class="st">&#39;new_cohort_table1&#39;</span>,
                                                               <span class="st">&#39;new_cohort_table2&#39;</span>),
                                  <span class="dt">validationTableOutcome =</span> <span class="kw">list</span>(<span class="st">&#39;new_cohort_table1&#39;</span>,
                                                                <span class="st">&#39;new_cohort_table2&#39;</span>),
                                  <span class="dt">validationIdTarget =</span> <span class="st">&#39;cohort_id&#39;</span>,
                                  <span class="dt">validationIdOutcome =</span> <span class="st">&#39;outcome_id&#39;</span>,
                                  <span class="dt">keepPrediction =</span> T
                                  )</code></pre></div>
</div>
<div id="journal-paper-generation" class="section level2">
<h2><span class="header-section-number">13.5</span> Journal paper generation</h2>
<p>We have added functionality to automatically generate a word document you can use as start of a journal paper. It contains many of the generated study details and results. If you have performed external validation these results will can be added as well. Optionally, you can add a “Table 1” that contains data on many covariates for the target population.</p>
<p>You can create the draft journal paper by running this function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> <span class="kw">createPlpJournalDocument</span>(<span class="dt">plpResult =</span> <span class="op">&lt;</span>your plp results<span class="op">&gt;</span>,
                          <span class="dt">plpValidation =</span> <span class="op">&lt;</span>your validation results<span class="op">&gt;</span>,
                          <span class="dt">plpData =</span> <span class="op">&lt;</span>your plp data<span class="op">&gt;</span>,
                          <span class="dt">targetName =</span> <span class="st">&quot;&lt;target population&gt;&quot;</span>,
                          <span class="dt">outcomeName =</span> <span class="st">&quot;&lt;outcome&gt;&quot;</span>,
                          <span class="dt">table1 =</span> F,
                          <span class="dt">connectionDetails =</span> <span class="ot">NULL</span>,
                          <span class="dt">includeTrain =</span> <span class="ot">FALSE</span>,
                          <span class="dt">includeTest =</span> <span class="ot">TRUE</span>,
                          <span class="dt">includePredictionPicture =</span> <span class="ot">TRUE</span>,
                          <span class="dt">includeAttritionPlot =</span> <span class="ot">TRUE</span>,
                          <span class="dt">outputLocation =</span> <span class="st">&quot;&lt;your location&gt;&quot;</span>)</code></pre></div>
<p>For more details see the help page of the function.</p>

</div>
<div id="other-functionality" class="section level2">
<h2><span class="header-section-number">13.6</span> Other functionality</h2>
<p>The package has much more functionality than described in this vignette and contributions have been made my many persons in the OHDSI community. The table below provides an overview:</p>
<table>
<colgroup>
<col width="20%" />
<col width="58%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th>Functionality</th>
<th>Description</th>
<th>Vignette</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Builing Multiple Models</td>
<td>This vignette describes how you can run multiple models automatically</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/BuildingMultiplePredictiveModels.pdf"><code>Vignette</code></a></td>
</tr>
<tr class="even">
<td>Custom algorithms</td>
<td>This vignette describes how you can add your own custom algorithms in the framework</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/AddingCustomAlgorithms.pdf"><code>Vignette</code></a></td>
</tr>
<tr class="odd">
<td>Ensemble models</td>
<td>This vignette describes how you can use the framework to build ensemble models, i.e combine multiple models in a super learner</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/BuildingEnsembleModels.pdf"><code>Vignette</code></a></td>
</tr>
<tr class="even">
<td>Deep Learning Models</td>
<td>We have added extensive functionality for Deep Learning including several architectures in both pyTorch and Keras. These algorithms can be trained using GPU power</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/BuildingDeepLearningModels.pdf"><code>Vignette</code></a></td>
</tr>
<tr class="odd">
<td>Learning curves</td>
<td>Learning curves assess the effect of training set size on model performance by training a sequence of prediction models on successively larger subsets of the training set. A learning curve plot can also help in diagnosing a bias or variance problem as explained below.</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/GeneratingLearningCurves.pdf"><code>Vignette</code></a></td>
</tr>
<tr class="even">
<td>Implementing existing models</td>
<td>This vignette describes how you can implement existing logistic regression models in the framework, e.g. as found in literature</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/ImplementingExistingModels.pdf"><code>Vignette</code></a></td>
</tr>
</tbody>
</table>
</div>
<div id="demos" class="section level2">
<h2><span class="header-section-number">13.7</span> Demos</h2>
<p>We have added several demos in the package that run on simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Show all demos in our package:</span>
 <span class="kw">demo</span>(<span class="dt">package =</span> <span class="st">&quot;PatientLevelPrediction&quot;</span>)

<span class="co"># For example, to run the SingleModelDemo that runs Lasso and shows you how to run the Shiny App use this call</span>
 <span class="kw">demo</span>(<span class="st">&quot;SingleModelDemo&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;PatientLevelPrediction&quot;</span>)</code></pre></div>

</div>
<div id="acknowledgments" class="section level2">
<h2><span class="header-section-number">13.8</span> Acknowledgments</h2>
<p>Considerable work has been dedicated to provide the <a href="http://github.com/OHDSI/PatientLevelPrediction"><code>PatientLevelPrediction</code></a> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># citation(&quot;PatientLevelPrediction&quot;)</span></code></pre></div>
<p>Further, <a href="http://github.com/OHDSI/PatientLevelPrediction"><code>PatientLevelPrediction</code></a> makes extensive use of the <a href="http://github.com/OHDSI/Cyclops"><code>Cyclops</code></a> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># citation(&quot;Cyclops&quot;)</span></code></pre></div>
<p><strong>Please reference this paper if you use the PLP Package in your work:</strong></p>
<p><a href="http://dx.doi.org/10.1093/jamia/ocy032">Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data. J Am Med Inform Assoc. 2018;25(8):969-975.</a></p>
<p>This work is supported in part through the National Science Foundation grant IIS 1251151 and a personalised grant to P.R. Rijnbeek from Janssen R&amp;D.</p>

</div>
<div id="appendix-1-study-population-settings-details" class="section level2 unnumbered">
<h2>Appendix 1: Study population settings details</h2>
<p>In the figures below the effect is shown of the removeSubjectsWithPriorOutcome, requireTimAtRisk, and includeAllOutcomes booleans on the final study population. We start with a Target Cohort with firstExposureOnly = false and we require a washout period = 1095. We then subset the target cohort based on additional constraints. The final study population in the Venn diagrams below are colored green.</p>
<ol style="list-style-type: decimal">
<li>Require minimum time-at-risk for all person in the target cohort.</li>
</ol>
<p><img src="images/PatientLevelPrediction/popdef1.png" width="100%" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Require minumum time-at-risk for target cohort, except for persons with outcomes during time-at-risk.</li>
</ol>
<p><img src="images/PatientLevelPrediction/popdef2.png" width="100%" /></p>

<ol start="3" style="list-style-type: decimal">
<li>Include all persons in the target cohort exclude persons with prior outcomes.</li>
</ol>
<p><img src="images/PatientLevelPrediction/popdef3.png" width="100%" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Require minimum time-at-risk for target cohort, except for persons with outcomes during time-at-risk, exclude persons with prior outcomes.</li>
</ol>
<p><img src="images/PatientLevelPrediction/popdef4.png" width="100%" /></p>

<ol start="5" style="list-style-type: decimal">
<li>Include all persons in target cohort exclude persons with prior outcomes.</li>
</ol>
<p><img src="images/PatientLevelPrediction/popdef5.png" width="100%" /></p>
<ol start="6" style="list-style-type: decimal">
<li>Include all persons in target cohort.</li>
</ol>
<p><img src="images/PatientLevelPrediction/popdef6.png" width="100%" /></p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-nguyen2008">
<p>Nguyen, N. D., S. A. Frost, J. R. Center, J. A. Eisman, and T. V. Nguyen. 2008. “Development of prognostic nomograms for individualizing 5-year and 10-year fracture risks.” <em>Osteoporos Int</em> 19 (10): 1431–44.</p>
</div>
<div id="ref-lee1995">
<p>Lee, K. L., L. H. Woodlief, E. J. Topol, W. D. Weaver, A. Betriu, J. Col, M. Simoons, P. Aylward, F. Van de Werf, and R. M. Califf. 1995. “Predictors of 30-day mortality in the era of reperfusion for acute myocardial infarction. Results from an international trial of 41,021 patients. GUSTO-I Investigators.” <em>Circulation</em> 91 (6): 1659–68.</p>
</div>
<div id="ref-wilson1998">
<p>Wilson, P. W., R. B. D’Agostino, D. Levy, A. M. Belanger, H. Silbershatz, and W. B. Kannel. 1998. “Prediction of coronary heart disease using risk factor categories.” <em>Circulation</em> 97 (18): 1837–47.</p>
</div>
<div id="ref-engel2015">
<p>Engel, C., and C. Fischer. 2015. “Breast cancer risks and risk prediction models.” <em>Breast Care (Basel)</em> 10 (1): 7–12.</p>
</div>
<div id="ref-perel2006">
<p>Perel, P., P. Edwards, R. Wentz, and I. Roberts. 2006. “Systematic review of prognostic models in traumatic brain injury.” <em>BMC Med Inform Decis Mak</em> 6 (November): 38.</p>
</div>
<div id="ref-reps2018">
<p>Reps, J. M., M. J. Schuemie, M. A. Suchard, P. B. Ryan, and P. R. Rijnbeek. 2018. “Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data.” <em>Journal of the American Medical Informatics Association</em> 25 (8): 969–75. doi:<a href="https://doi.org/10.1093/jamia/ocy032">10.1093/jamia/ocy032</a>.</p>
</div>
<div id="ref-gage2001">
<p>Gage, B. F., A. D. Waterman, W. Shannon, M. Boechler, M. W. Rich, and M. J. Radford. 2001. “Validation of clinical classification schemes for predicting stroke: results from the National Registry of Atrial Fibrillation.” <em>JAMA</em> 285 (22): 2864–70.</p>
</div>
<div id="ref-keogh2011">
<p>Keogh, C., E. Wallace, C. Dillon, B. D. Dimitrov, and T. Fahey. 2011. “Validation of the CHADS2 clinical prediction rule to predict ischaemic stroke. A systematic review and meta-analysis.” <em>Thromb. Haemost.</em> 106 (3): 528–38.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="PopulationLevelEstimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="StudySteps.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/OHDSI/TheBookOfOhdsi/edit/master/PatientLevelPrediction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["TheBookOfOhdsi.pdf", "TheBookOfOhdsi.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
