<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 19 Method Validity | The Book of OHDSI</title>
  <meta name="description" content="A book about the Observational Health Data Science and Informatics (OHDS). It described the OHDSI community, open standards and open source software." />
  <meta name="generator" content="bookdown 0.11.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 19 Method Validity | The Book of OHDSI" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ohdsi.github.io/TheBookOfOhdsi/" />
  <meta property="og:image" content="https://ohdsi.github.io/TheBookOfOhdsi/images/Cover/Cover.png" />
  <meta property="og:description" content="A book about the Observational Health Data Science and Informatics (OHDS). It described the OHDSI community, open standards and open source software." />
  <meta name="github-repo" content="OHDSI/TheBookOfOhdsi" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 19 Method Validity | The Book of OHDSI" />
  
  <meta name="twitter:description" content="A book about the Observational Health Data Science and Informatics (OHDS). It described the OHDSI community, open standards and open source software." />
  <meta name="twitter:image" content="https://ohdsi.github.io/TheBookOfOhdsi/images/Cover/Cover.png" />

<meta name="author" content="Observational Health Data Science and Informatics" />


<meta name="date" content="2019-07-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="SoftwareValidity.html">
<link rel="next" href="StudySteps.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-104086677-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-104086677-2');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Book of OHDSI</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#goals-of-this-book"><i class="fa fa-check"></i>Goals of this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
</ul></li>
<li class="part"><span><b>I The OHDSI Community</b></span></li>
<li class="chapter" data-level="1" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html"><i class="fa fa-check"></i><b>1</b> Mission, vision, values</a><ul>
<li class="chapter" data-level="1.1" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html#our-mission"><i class="fa fa-check"></i><b>1.1</b> Our Mission</a></li>
<li class="chapter" data-level="1.2" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html#our-vision"><i class="fa fa-check"></i><b>1.2</b> Our Vision</a></li>
<li class="chapter" data-level="1.3" data-path="MissionVissionValues.html"><a href="MissionVissionValues.html#our-objectives"><i class="fa fa-check"></i><b>1.3</b> Our Objectives</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Collaborators.html"><a href="Collaborators.html"><i class="fa fa-check"></i><b>2</b> Collaborators</a></li>
<li class="chapter" data-level="3" data-path="OpenScience.html"><a href="OpenScience.html"><i class="fa fa-check"></i><b>3</b> Open Science</a><ul>
<li class="chapter" data-level="3.1" data-path="OpenScience.html"><a href="OpenScience.html#open-science"><i class="fa fa-check"></i><b>3.1</b> Open Science</a></li>
<li class="chapter" data-level="3.2" data-path="OpenScience.html"><a href="OpenScience.html#fair-guiding-principles"><i class="fa fa-check"></i><b>3.2</b> FAIR Guiding Principles</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="WhereToBegin.html"><a href="WhereToBegin.html"><i class="fa fa-check"></i><b>4</b> Where to begin</a></li>
<li class="part"><span><b>II Uniform Data Representation</b></span></li>
<li class="chapter" data-level="5" data-path="CommonDataModel.html"><a href="CommonDataModel.html"><i class="fa fa-check"></i><b>5</b> The Common Data Model</a><ul>
<li class="chapter" data-level="5.1" data-path="CommonDataModel.html"><a href="CommonDataModel.html#design-principles"><i class="fa fa-check"></i><b>5.1</b> Design Principles</a></li>
<li class="chapter" data-level="5.2" data-path="CommonDataModel.html"><a href="CommonDataModel.html#data-model-conventions"><i class="fa fa-check"></i><b>5.2</b> Data Model Conventions</a><ul>
<li class="chapter" data-level="5.2.1" data-path="CommonDataModel.html"><a href="CommonDataModel.html#model-conv"><i class="fa fa-check"></i><b>5.2.1</b> General conventions of the model</a></li>
<li class="chapter" data-level="5.2.2" data-path="CommonDataModel.html"><a href="CommonDataModel.html#general-conventions-of-schemas"><i class="fa fa-check"></i><b>5.2.2</b> General conventions of schemas</a></li>
<li class="chapter" data-level="5.2.3" data-path="CommonDataModel.html"><a href="CommonDataModel.html#general-conventions-of-data-tables"><i class="fa fa-check"></i><b>5.2.3</b> General conventions of data tables</a></li>
<li class="chapter" data-level="5.2.4" data-path="CommonDataModel.html"><a href="CommonDataModel.html#general-conventions-of-fields"><i class="fa fa-check"></i><b>5.2.4</b> General conventions of fields</a></li>
<li class="chapter" data-level="5.2.5" data-path="CommonDataModel.html"><a href="CommonDataModel.html#representation-of-content-through-concepts"><i class="fa fa-check"></i><b>5.2.5</b> Representation of content through Concepts</a></li>
<li class="chapter" data-level="5.2.6" data-path="CommonDataModel.html"><a href="CommonDataModel.html#difference-between-concept-ids-and-source-values"><i class="fa fa-check"></i><b>5.2.6</b> Difference between Concept IDs and Source Values</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="CommonDataModel.html"><a href="CommonDataModel.html#omop-cdm-standardized-tables"><i class="fa fa-check"></i><b>5.3</b> OMOP CDM Standardized Tables</a><ul>
<li class="chapter" data-level="5.3.1" data-path="CommonDataModel.html"><a href="CommonDataModel.html#running-example-endometriosis"><i class="fa fa-check"></i><b>5.3.1</b> Running Example: Endometriosis</a></li>
<li class="chapter" data-level="5.3.2" data-path="CommonDataModel.html"><a href="CommonDataModel.html#person"><i class="fa fa-check"></i><b>5.3.2</b> PERSON table</a></li>
<li class="chapter" data-level="5.3.3" data-path="CommonDataModel.html"><a href="CommonDataModel.html#observationPeriod"><i class="fa fa-check"></i><b>5.3.3</b> OBSERVATION_PERIOD table</a></li>
<li class="chapter" data-level="5.3.4" data-path="CommonDataModel.html"><a href="CommonDataModel.html#visitOccurrence"><i class="fa fa-check"></i><b>5.3.4</b> VISIT_OCCURRENCE</a></li>
<li class="chapter" data-level="5.3.5" data-path="CommonDataModel.html"><a href="CommonDataModel.html#conditionOccurrence"><i class="fa fa-check"></i><b>5.3.5</b> CONDITION_OCCURRENCE</a></li>
<li class="chapter" data-level="5.3.6" data-path="CommonDataModel.html"><a href="CommonDataModel.html#drugExposure"><i class="fa fa-check"></i><b>5.3.6</b> DRUG_EXPOSURE</a></li>
<li class="chapter" data-level="5.3.7" data-path="CommonDataModel.html"><a href="CommonDataModel.html#procedureOccurrence"><i class="fa fa-check"></i><b>5.3.7</b> PROCEDURE_OCCURRENCE</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="CommonDataModel.html"><a href="CommonDataModel.html#summary"><i class="fa fa-check"></i><b>5.4</b> Summary</a></li>
<li class="chapter" data-level="5.5" data-path="CommonDataModel.html"><a href="CommonDataModel.html#exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="StandardizedVocabularies.html"><a href="StandardizedVocabularies.html"><i class="fa fa-check"></i><b>6</b> Standardized Vocabularies</a></li>
<li class="chapter" data-level="7" data-path="ExtractTransformLoad.html"><a href="ExtractTransformLoad.html"><i class="fa fa-check"></i><b>7</b> Extract Transform Load</a></li>
<li class="part"><span><b>III Data Analytics</b></span></li>
<li class="chapter" data-level="8" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html"><i class="fa fa-check"></i><b>8</b> Data Analytics Use Cases</a><ul>
<li class="chapter" data-level="8.1" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html#characterization"><i class="fa fa-check"></i><b>8.1</b> Characterization</a></li>
<li class="chapter" data-level="8.2" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html#population-level-estimation"><i class="fa fa-check"></i><b>8.2</b> Population-level estimation</a></li>
<li class="chapter" data-level="8.3" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html#patient-level-prediction"><i class="fa fa-check"></i><b>8.3</b> Patient-Level prediction</a></li>
<li class="chapter" data-level="8.4" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html#limitations-of-observational-research"><i class="fa fa-check"></i><b>8.4</b> Limitations of observational research</a><ul>
<li class="chapter" data-level="8.4.1" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html#missing-data"><i class="fa fa-check"></i><b>8.4.1</b> Missing data</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="DataAnalyticsUseCases.html"><a href="DataAnalyticsUseCases.html#summary-1"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html"><i class="fa fa-check"></i><b>9</b> OHDSI Analytics Tools</a><ul>
<li class="chapter" data-level="9.1" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#analysis-implementation"><i class="fa fa-check"></i><b>9.1</b> Analysis implementation</a></li>
<li class="chapter" data-level="9.2" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#analysis-strategy"><i class="fa fa-check"></i><b>9.2</b> Analysis strategy</a></li>
<li class="chapter" data-level="9.3" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#atlas"><i class="fa fa-check"></i><b>9.3</b> ATLAS</a><ul>
<li class="chapter" data-level="9.3.1" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#security"><i class="fa fa-check"></i><b>9.3.1</b> Security</a></li>
<li class="chapter" data-level="9.3.2" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#documentation"><i class="fa fa-check"></i><b>9.3.2</b> Documentation</a></li>
<li class="chapter" data-level="9.3.3" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#system-requirements"><i class="fa fa-check"></i><b>9.3.3</b> System requirements</a></li>
<li class="chapter" data-level="9.3.4" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#how-to-install"><i class="fa fa-check"></i><b>9.3.4</b> How to install</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#methods-library"><i class="fa fa-check"></i><b>9.4</b> Methods Library</a><ul>
<li class="chapter" data-level="9.4.1" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#support-for-large-scale-analytics"><i class="fa fa-check"></i><b>9.4.1</b> Support for large-scale analytics</a></li>
<li class="chapter" data-level="9.4.2" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#BigDataSupport"><i class="fa fa-check"></i><b>9.4.2</b> Support for big data</a></li>
<li class="chapter" data-level="9.4.3" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#documentation-1"><i class="fa fa-check"></i><b>9.4.3</b> Documentation</a></li>
<li class="chapter" data-level="9.4.4" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#system-requirements-1"><i class="fa fa-check"></i><b>9.4.4</b> System requirements</a></li>
<li class="chapter" data-level="9.4.5" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#how-to-install-1"><i class="fa fa-check"></i><b>9.4.5</b> How to install</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#installing-java"><i class="fa fa-check"></i><b>9.5</b> Installing Java</a></li>
<li class="chapter" data-level="9.6" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#deployment-strategies"><i class="fa fa-check"></i><b>9.6</b> Deployment strategies</a><ul>
<li class="chapter" data-level="9.6.1" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#broadsea"><i class="fa fa-check"></i><b>9.6.1</b> Broadsea</a></li>
<li class="chapter" data-level="9.6.2" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#amazon-aws"><i class="fa fa-check"></i><b>9.6.2</b> Amazon AWS</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#summary-2"><i class="fa fa-check"></i><b>9.7</b> Summary</a></li>
<li class="chapter" data-level="9.8" data-path="OhdsiAnalyticsTools.html"><a href="OhdsiAnalyticsTools.html#exercises-1"><i class="fa fa-check"></i><b>9.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="SqlAndR.html"><a href="SqlAndR.html"><i class="fa fa-check"></i><b>10</b> SQL and R</a><ul>
<li class="chapter" data-level="10.1" data-path="SqlAndR.html"><a href="SqlAndR.html#SqlRender"><i class="fa fa-check"></i><b>10.1</b> SqlRender</a><ul>
<li class="chapter" data-level="10.1.1" data-path="SqlAndR.html"><a href="SqlAndR.html#sql-parameterization"><i class="fa fa-check"></i><b>10.1.1</b> SQL parameterization</a></li>
<li class="chapter" data-level="10.1.2" data-path="SqlAndR.html"><a href="SqlAndR.html#translation-to-other-sql-dialects"><i class="fa fa-check"></i><b>10.1.2</b> Translation to other SQL dialects</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="SqlAndR.html"><a href="SqlAndR.html#DatabaseConnector"><i class="fa fa-check"></i><b>10.2</b> DatabaseConnector</a><ul>
<li class="chapter" data-level="10.2.1" data-path="SqlAndR.html"><a href="SqlAndR.html#creating-a-connection"><i class="fa fa-check"></i><b>10.2.1</b> Creating a connection</a></li>
<li class="chapter" data-level="10.2.2" data-path="SqlAndR.html"><a href="SqlAndR.html#querying"><i class="fa fa-check"></i><b>10.2.2</b> Querying</a></li>
<li class="chapter" data-level="10.2.3" data-path="SqlAndR.html"><a href="SqlAndR.html#querying-using-ffdf-objects"><i class="fa fa-check"></i><b>10.2.3</b> Querying using ffdf objects</a></li>
<li class="chapter" data-level="10.2.4" data-path="SqlAndR.html"><a href="SqlAndR.html#querying-different-platforms-using-the-same-sql"><i class="fa fa-check"></i><b>10.2.4</b> Querying different platforms using the same SQL</a></li>
<li class="chapter" data-level="10.2.5" data-path="SqlAndR.html"><a href="SqlAndR.html#inserting-tables"><i class="fa fa-check"></i><b>10.2.5</b> Inserting tables</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="SqlAndR.html"><a href="SqlAndR.html#QueryTheCdm"><i class="fa fa-check"></i><b>10.3</b> Querying the CDM</a></li>
<li class="chapter" data-level="10.4" data-path="SqlAndR.html"><a href="SqlAndR.html#using-the-vocabulary-when-querying"><i class="fa fa-check"></i><b>10.4</b> Using the vocabulary when querying</a></li>
<li class="chapter" data-level="10.5" data-path="SqlAndR.html"><a href="SqlAndR.html#querylibrary"><i class="fa fa-check"></i><b>10.5</b> QueryLibrary</a></li>
<li class="chapter" data-level="10.6" data-path="SqlAndR.html"><a href="SqlAndR.html#designing-a-simple-study"><i class="fa fa-check"></i><b>10.6</b> Designing a simple study</a><ul>
<li class="chapter" data-level="10.6.1" data-path="SqlAndR.html"><a href="SqlAndR.html#problem-definition"><i class="fa fa-check"></i><b>10.6.1</b> Problem definition</a></li>
<li class="chapter" data-level="10.6.2" data-path="SqlAndR.html"><a href="SqlAndR.html#exposure"><i class="fa fa-check"></i><b>10.6.2</b> Exposure</a></li>
<li class="chapter" data-level="10.6.3" data-path="SqlAndR.html"><a href="SqlAndR.html#outcome"><i class="fa fa-check"></i><b>10.6.3</b> Outcome</a></li>
<li class="chapter" data-level="10.6.4" data-path="SqlAndR.html"><a href="SqlAndR.html#time-at-risk"><i class="fa fa-check"></i><b>10.6.4</b> Time-at-risk</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="SqlAndR.html"><a href="SqlAndR.html#implementing-the-study-using-sql-and-r"><i class="fa fa-check"></i><b>10.7</b> Implementing the study using SQL and R</a><ul>
<li class="chapter" data-level="10.7.1" data-path="SqlAndR.html"><a href="SqlAndR.html#exposure-cohort"><i class="fa fa-check"></i><b>10.7.1</b> Exposure cohort</a></li>
<li class="chapter" data-level="10.7.2" data-path="SqlAndR.html"><a href="SqlAndR.html#outcome-cohort"><i class="fa fa-check"></i><b>10.7.2</b> Outcome cohort</a></li>
<li class="chapter" data-level="10.7.3" data-path="SqlAndR.html"><a href="SqlAndR.html#incidence-rate-calculation"><i class="fa fa-check"></i><b>10.7.3</b> Incidence rate calculation</a></li>
<li class="chapter" data-level="10.7.4" data-path="SqlAndR.html"><a href="SqlAndR.html#clean-up"><i class="fa fa-check"></i><b>10.7.4</b> Clean up</a></li>
<li class="chapter" data-level="10.7.5" data-path="SqlAndR.html"><a href="SqlAndR.html#compatibility"><i class="fa fa-check"></i><b>10.7.5</b> Compatibility</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="SqlAndR.html"><a href="SqlAndR.html#summary-3"><i class="fa fa-check"></i><b>10.8</b> Summary</a></li>
<li class="chapter" data-level="10.9" data-path="SqlAndR.html"><a href="SqlAndR.html#exercises-2"><i class="fa fa-check"></i><b>10.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Cohorts.html"><a href="Cohorts.html"><i class="fa fa-check"></i><b>11</b> Building the building blocks: cohorts</a></li>
<li class="chapter" data-level="12" data-path="Characterization.html"><a href="Characterization.html"><i class="fa fa-check"></i><b>12</b> Characterization</a></li>
<li class="chapter" data-level="13" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html"><i class="fa fa-check"></i><b>13</b> Population-level estimation</a><ul>
<li class="chapter" data-level="13.1" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#CohortMethod"><i class="fa fa-check"></i><b>13.1</b> The cohort method design</a><ul>
<li class="chapter" data-level="13.1.1" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#propensity-scores"><i class="fa fa-check"></i><b>13.1.1</b> Propensity scores</a></li>
<li class="chapter" data-level="13.1.2" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#VariableSelection"><i class="fa fa-check"></i><b>13.1.2</b> Variable selection</a></li>
<li class="chapter" data-level="13.1.3" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#caliper"><i class="fa fa-check"></i><b>13.1.3</b> Caliper</a></li>
<li class="chapter" data-level="13.1.4" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#overlap-preference-scores"><i class="fa fa-check"></i><b>13.1.4</b> Overlap: preference scores</a></li>
<li class="chapter" data-level="13.1.5" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#balance"><i class="fa fa-check"></i><b>13.1.5</b> Balance</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#the-self-controlled-cohort-design"><i class="fa fa-check"></i><b>13.2</b> The self-controlled cohort design</a></li>
<li class="chapter" data-level="13.3" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#the-case-control-design"><i class="fa fa-check"></i><b>13.3</b> The case-control design</a></li>
<li class="chapter" data-level="13.4" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#the-case-crossover-design"><i class="fa fa-check"></i><b>13.4</b> The case-crossover design</a></li>
<li class="chapter" data-level="13.5" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#the-self-controlled-case-series-design"><i class="fa fa-check"></i><b>13.5</b> The self-controlled case series design</a></li>
<li class="chapter" data-level="13.6" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#designing-a-hypertension-study"><i class="fa fa-check"></i><b>13.6</b> Designing a hypertension study</a><ul>
<li class="chapter" data-level="13.6.1" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#problem-definition-1"><i class="fa fa-check"></i><b>13.6.1</b> Problem definition</a></li>
<li class="chapter" data-level="13.6.2" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#target-and-comparator"><i class="fa fa-check"></i><b>13.6.2</b> Target and comparator</a></li>
<li class="chapter" data-level="13.6.3" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#outcome-1"><i class="fa fa-check"></i><b>13.6.3</b> Outcome</a></li>
<li class="chapter" data-level="13.6.4" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#time-at-risk-1"><i class="fa fa-check"></i><b>13.6.4</b> Time-at-risk</a></li>
<li class="chapter" data-level="13.6.5" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#model"><i class="fa fa-check"></i><b>13.6.5</b> Model</a></li>
<li class="chapter" data-level="13.6.6" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#study-summary"><i class="fa fa-check"></i><b>13.6.6</b> Study summary</a></li>
<li class="chapter" data-level="13.6.7" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#control-questions"><i class="fa fa-check"></i><b>13.6.7</b> Control questions</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#PleAtlas"><i class="fa fa-check"></i><b>13.7</b> Implementing the study using ATLAS</a><ul>
<li class="chapter" data-level="13.7.1" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#ComparisonSettings"><i class="fa fa-check"></i><b>13.7.1</b> Comparative cohort settings</a></li>
<li class="chapter" data-level="13.7.2" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#effect-estimation-analysis-settings"><i class="fa fa-check"></i><b>13.7.2</b> Effect estimation analysis settings</a></li>
<li class="chapter" data-level="13.7.3" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#evaluationSettings"><i class="fa fa-check"></i><b>13.7.3</b> Evaluation settings</a></li>
<li class="chapter" data-level="13.7.4" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#running-the-study-package"><i class="fa fa-check"></i><b>13.7.4</b> Running the study package</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#pleR"><i class="fa fa-check"></i><b>13.8</b> Implementing the study using R</a><ul>
<li class="chapter" data-level="13.8.1" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#cohort-instantiation"><i class="fa fa-check"></i><b>13.8.1</b> Cohort instantiation</a></li>
<li class="chapter" data-level="13.8.2" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#data-extraction"><i class="fa fa-check"></i><b>13.8.2</b> Data extraction</a></li>
<li class="chapter" data-level="13.8.3" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#defining-the-study-population"><i class="fa fa-check"></i><b>13.8.3</b> Defining the study population</a></li>
<li class="chapter" data-level="13.8.4" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#propensity-scores-1"><i class="fa fa-check"></i><b>13.8.4</b> Propensity scores</a></li>
<li class="chapter" data-level="13.8.5" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#outcome-models"><i class="fa fa-check"></i><b>13.8.5</b> Outcome models</a></li>
<li class="chapter" data-level="13.8.6" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#MultipleAnalyses"><i class="fa fa-check"></i><b>13.8.6</b> Running multiple analyses</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#studyOutputs"><i class="fa fa-check"></i><b>13.9</b> Study outputs</a><ul>
<li class="chapter" data-level="13.9.1" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#propensity-scores-and-model"><i class="fa fa-check"></i><b>13.9.1</b> Propensity scores and model</a></li>
<li class="chapter" data-level="13.9.2" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#covariate-balance"><i class="fa fa-check"></i><b>13.9.2</b> Covariate balance</a></li>
<li class="chapter" data-level="13.9.3" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#follow-up-and-power"><i class="fa fa-check"></i><b>13.9.3</b> Follow up and power</a></li>
<li class="chapter" data-level="13.9.4" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#kaplan-meier"><i class="fa fa-check"></i><b>13.9.4</b> Kaplan Meier</a></li>
<li class="chapter" data-level="13.9.5" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#effect-size-estimate"><i class="fa fa-check"></i><b>13.9.5</b> Effect size estimate</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#summary-4"><i class="fa fa-check"></i><b>13.10</b> Summary</a></li>
<li class="chapter" data-level="13.11" data-path="PopulationLevelEstimation.html"><a href="PopulationLevelEstimation.html#excercises"><i class="fa fa-check"></i><b>13.11</b> Excercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html"><i class="fa fa-check"></i><b>14</b> Patient Level Prediction</a><ul>
<li class="chapter" data-level="14.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#introduction"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#progress"><i class="fa fa-check"></i><b>14.2</b> Current Progress in Patient-Level Prediction</a></li>
<li class="chapter" data-level="14.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#extracted"><i class="fa fa-check"></i><b>14.3</b> Creating Labelled Data</a></li>
<li class="chapter" data-level="14.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#supervised"><i class="fa fa-check"></i><b>14.4</b> Supervised learning</a><ul>
<li class="chapter" data-level="14.4.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>14.4.1</b> Regularized Logistic Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>14.4.2</b> Gradient boosting machines</a></li>
<li class="chapter" data-level="14.4.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#random-forest"><i class="fa fa-check"></i><b>14.4.3</b> Random forest</a></li>
<li class="chapter" data-level="14.4.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>14.4.4</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="14.4.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#naive-bayes"><i class="fa fa-check"></i><b>14.4.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="14.4.6" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#adaboost"><i class="fa fa-check"></i><b>14.4.6</b> AdaBoost</a></li>
<li class="chapter" data-level="14.4.7" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#decision-tree"><i class="fa fa-check"></i><b>14.4.7</b> Decision Tree</a></li>
<li class="chapter" data-level="14.4.8" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#multilayer-perception"><i class="fa fa-check"></i><b>14.4.8</b> Multilayer Perception</a></li>
<li class="chapter" data-level="14.4.9" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#deep-learning"><i class="fa fa-check"></i><b>14.4.9</b> Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#evalaution"><i class="fa fa-check"></i><b>14.5</b> Evaluating Patient-Level Prediction Models</a><ul>
<li class="chapter" data-level="14.5.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#evaluation-types"><i class="fa fa-check"></i><b>14.5.1</b> Evaluation Types</a></li>
<li class="chapter" data-level="14.5.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#performance"><i class="fa fa-check"></i><b>14.5.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="14.5.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#inspecting-the-model"><i class="fa fa-check"></i><b>14.5.3</b> Inspecting the model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#specfifying-a-patient-level-prediction-study"><i class="fa fa-check"></i><b>14.6</b> Specfifying a Patient-level Prediction Study</a><ul>
<li class="chapter" data-level="14.6.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#problem-definition-2"><i class="fa fa-check"></i><b>14.6.1</b> Problem definition</a></li>
<li class="chapter" data-level="14.6.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-population-definition"><i class="fa fa-check"></i><b>14.6.2</b> Study population definition</a></li>
<li class="chapter" data-level="14.6.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#model-development-settings"><i class="fa fa-check"></i><b>14.6.3</b> Model development settings</a></li>
<li class="chapter" data-level="14.6.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#model-evaluation"><i class="fa fa-check"></i><b>14.6.4</b> Model evaluation</a></li>
<li class="chapter" data-level="14.6.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#study-summary-1"><i class="fa fa-check"></i><b>14.6.5</b> Study summary</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#implementing-the-study-in-atlas"><i class="fa fa-check"></i><b>14.7</b> Implementing the study in Atlas</a><ul>
<li class="chapter" data-level="14.7.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#introduction-1"><i class="fa fa-check"></i><b>14.7.1</b> Introduction</a></li>
<li class="chapter" data-level="14.7.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#the-atlas-layout"><i class="fa fa-check"></i><b>14.7.2</b> The Atlas layout</a></li>
<li class="chapter" data-level="14.7.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#atlas-specification-tab"><i class="fa fa-check"></i><b>14.7.3</b> Atlas Specification Tab</a></li>
<li class="chapter" data-level="14.7.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#prediction-problem-settings"><i class="fa fa-check"></i><b>14.7.4</b> Prediction Problem Settings</a></li>
<li class="chapter" data-level="14.7.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#analysis-settings"><i class="fa fa-check"></i><b>14.7.5</b> Analysis Settings</a></li>
<li class="chapter" data-level="14.7.6" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#model-settings"><i class="fa fa-check"></i><b>14.7.6</b> Model Settings</a></li>
<li class="chapter" data-level="14.7.7" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#covariate-settings"><i class="fa fa-check"></i><b>14.7.7</b> Covariate Settings</a></li>
<li class="chapter" data-level="14.7.8" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#population-settings"><i class="fa fa-check"></i><b>14.7.8</b> Population Settings</a></li>
<li class="chapter" data-level="14.7.9" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#execution-settings"><i class="fa fa-check"></i><b>14.7.9</b> Execution settings</a></li>
<li class="chapter" data-level="14.7.10" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#training-settings"><i class="fa fa-check"></i><b>14.7.10</b> Training settings</a></li>
<li class="chapter" data-level="14.7.11" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#atlas-utilities-tab"><i class="fa fa-check"></i><b>14.7.11</b> Atlas Utilities Tab</a></li>
<li class="chapter" data-level="14.7.12" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#how-to-importexport-study"><i class="fa fa-check"></i><b>14.7.12</b> How to import/export study</a></li>
<li class="chapter" data-level="14.7.13" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#how-to-download-package"><i class="fa fa-check"></i><b>14.7.13</b> How to download package</a></li>
<li class="chapter" data-level="14.7.14" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#building-atlas-created-prediction-study-r-package"><i class="fa fa-check"></i><b>14.7.14</b> Building Atlas created prediction study R package</a></li>
<li class="chapter" data-level="14.7.15" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#running-study"><i class="fa fa-check"></i><b>14.7.15</b> Running Study</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#implementing-the-study-in-r"><i class="fa fa-check"></i><b>14.8</b> Implementing the study in R</a><ul>
<li class="chapter" data-level="14.8.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#cohort-instantiation-1"><i class="fa fa-check"></i><b>14.8.1</b> Cohort instantiation</a></li>
<li class="chapter" data-level="14.8.2" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#data-extraction-1"><i class="fa fa-check"></i><b>14.8.2</b> Data extraction</a></li>
<li class="chapter" data-level="14.8.3" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#additional-inclusion-criteria"><i class="fa fa-check"></i><b>14.8.3</b> Additional inclusion criteria</a></li>
<li class="chapter" data-level="14.8.4" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#model-development"><i class="fa fa-check"></i><b>14.8.4</b> Model Development</a></li>
<li class="chapter" data-level="14.8.5" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#internal-validation"><i class="fa fa-check"></i><b>14.8.5</b> Internal Validation</a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#exploring-a-single-plp-shiny-app"><i class="fa fa-check"></i><b>14.9</b> Exploring a single PLP Shiny App</a></li>
<li class="chapter" data-level="14.10" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#exploring-the-atlas-plp-shiny-app"><i class="fa fa-check"></i><b>14.10</b> Exploring the Atlas PLP Shiny App</a></li>
<li class="chapter" data-level="14.11" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#additional-patient-level-prediction-features"><i class="fa fa-check"></i><b>14.11</b> Additional Patient-level Prediction Features</a><ul>
<li class="chapter" data-level="14.11.1" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#journal-paper-generation"><i class="fa fa-check"></i><b>14.11.1</b> Journal paper generation</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="PatientLevelPrediction.html"><a href="PatientLevelPrediction.html#excercises-1"><i class="fa fa-check"></i><b>14.12</b> Excercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence Quality</b></span></li>
<li class="chapter" data-level="15" data-path="EvidenceQuality.html"><a href="EvidenceQuality.html"><i class="fa fa-check"></i><b>15</b> Introduction to Evidence Quality</a></li>
<li class="chapter" data-level="16" data-path="DataQuality.html"><a href="DataQuality.html"><i class="fa fa-check"></i><b>16</b> Data Quality</a><ul>
<li class="chapter" data-level="16.1" data-path="DataQuality.html"><a href="DataQuality.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="DataQuality.html"><a href="DataQuality.html#achilles-heel-tool"><i class="fa fa-check"></i><b>16.2</b> Achilles Heel tool</a><ul>
<li class="chapter" data-level="16.2.1" data-path="DataQuality.html"><a href="DataQuality.html#precomputed-analyses"><i class="fa fa-check"></i><b>16.2.1</b> Precomputed Analyses</a></li>
<li class="chapter" data-level="16.2.2" data-path="DataQuality.html"><a href="DataQuality.html#example-dq-check"><i class="fa fa-check"></i><b>16.2.2</b> Example DQ check</a></li>
<li class="chapter" data-level="16.2.3" data-path="DataQuality.html"><a href="DataQuality.html#overview-of-existing-dq-heel-checks"><i class="fa fa-check"></i><b>16.2.3</b> Overview of existing DQ Heel checks</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="DataQuality.html"><a href="DataQuality.html#study-specific-checks"><i class="fa fa-check"></i><b>16.3</b> Study-specific checks</a><ul>
<li class="chapter" data-level="16.3.1" data-path="DataQuality.html"><a href="DataQuality.html#outcomes"><i class="fa fa-check"></i><b>16.3.1</b> Outcomes</a></li>
<li class="chapter" data-level="16.3.2" data-path="DataQuality.html"><a href="DataQuality.html#laboratory-data"><i class="fa fa-check"></i><b>16.3.2</b> Laboratory data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ClinicalValidity.html"><a href="ClinicalValidity.html"><i class="fa fa-check"></i><b>17</b> Clinical Validity</a></li>
<li class="chapter" data-level="18" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html"><i class="fa fa-check"></i><b>18</b> Software Validity</a><ul>
<li class="chapter" data-level="18.1" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#software-development-process"><i class="fa fa-check"></i><b>18.1</b> Software Development Process</a><ul>
<li class="chapter" data-level="18.1.1" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#source-code-management"><i class="fa fa-check"></i><b>18.1.1</b> Source Code Management</a></li>
<li class="chapter" data-level="18.1.2" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#documentation-2"><i class="fa fa-check"></i><b>18.1.2</b> Documentation</a></li>
<li class="chapter" data-level="18.1.3" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#availability-of-current-and-historical-archive-versions"><i class="fa fa-check"></i><b>18.1.3</b> Availability of Current and Historical Archive Versions</a></li>
<li class="chapter" data-level="18.1.4" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#maintenance-support-and-retirement"><i class="fa fa-check"></i><b>18.1.4</b> Maintenance, Support and Retirement</a></li>
<li class="chapter" data-level="18.1.5" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#qualified-personnel"><i class="fa fa-check"></i><b>18.1.5</b> Qualified Personnel</a></li>
<li class="chapter" data-level="18.1.6" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#physical-and-logical-security"><i class="fa fa-check"></i><b>18.1.6</b> Physical and Logical Security</a></li>
<li class="chapter" data-level="18.1.7" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#disaster-recovery"><i class="fa fa-check"></i><b>18.1.7</b> Disaster Recovery</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#testing"><i class="fa fa-check"></i><b>18.2</b> Testing</a><ul>
<li class="chapter" data-level="18.2.1" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#unit-test"><i class="fa fa-check"></i><b>18.2.1</b> Unit test</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="SoftwareValidity.html"><a href="SoftwareValidity.html#conclusions"><i class="fa fa-check"></i><b>18.3</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="MethodValidity.html"><a href="MethodValidity.html"><i class="fa fa-check"></i><b>19</b> Method Validity</a><ul>
<li class="chapter" data-level="19.1" data-path="MethodValidity.html"><a href="MethodValidity.html#design-specific-diagnostics"><i class="fa fa-check"></i><b>19.1</b> Design-specific diagnostics</a></li>
<li class="chapter" data-level="19.2" data-path="MethodValidity.html"><a href="MethodValidity.html#diagnostics-for-all-estimation"><i class="fa fa-check"></i><b>19.2</b> Diagnostics for all estimation</a><ul>
<li class="chapter" data-level="19.2.1" data-path="MethodValidity.html"><a href="MethodValidity.html#NegativeControls"><i class="fa fa-check"></i><b>19.2.1</b> Negative controls</a></li>
<li class="chapter" data-level="19.2.2" data-path="MethodValidity.html"><a href="MethodValidity.html#PositiveControls"><i class="fa fa-check"></i><b>19.2.2</b> Positive controls</a></li>
<li class="chapter" data-level="19.2.3" data-path="MethodValidity.html"><a href="MethodValidity.html#metrics"><i class="fa fa-check"></i><b>19.2.3</b> Empirical evaluation</a></li>
<li class="chapter" data-level="19.2.4" data-path="MethodValidity.html"><a href="MethodValidity.html#p-value-calibration"><i class="fa fa-check"></i><b>19.2.4</b> P-value calibration</a></li>
<li class="chapter" data-level="19.2.5" data-path="MethodValidity.html"><a href="MethodValidity.html#confidence-interval-calibration"><i class="fa fa-check"></i><b>19.2.5</b> Confidence interval calibration</a></li>
<li class="chapter" data-level="19.2.6" data-path="MethodValidity.html"><a href="MethodValidity.html#replication-across-sites"><i class="fa fa-check"></i><b>19.2.6</b> Replication across sites</a></li>
<li class="chapter" data-level="19.2.7" data-path="MethodValidity.html"><a href="MethodValidity.html#sensitivity-analyses"><i class="fa fa-check"></i><b>19.2.7</b> Sensitivity analyses</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="MethodValidity.html"><a href="MethodValidity.html#method-validation-in-practice"><i class="fa fa-check"></i><b>19.3</b> Method validation in practice</a><ul>
<li class="chapter" data-level="19.3.1" data-path="MethodValidity.html"><a href="MethodValidity.html#selecting-negative-controls"><i class="fa fa-check"></i><b>19.3.1</b> Selecting negative controls</a></li>
<li class="chapter" data-level="19.3.2" data-path="MethodValidity.html"><a href="MethodValidity.html#including-controls"><i class="fa fa-check"></i><b>19.3.2</b> Including controls</a></li>
<li class="chapter" data-level="19.3.3" data-path="MethodValidity.html"><a href="MethodValidity.html#empirical-performance"><i class="fa fa-check"></i><b>19.3.3</b> Empirical performance</a></li>
<li class="chapter" data-level="19.3.4" data-path="MethodValidity.html"><a href="MethodValidity.html#p-value-calibration-1"><i class="fa fa-check"></i><b>19.3.4</b> P-value calibration</a></li>
<li class="chapter" data-level="19.3.5" data-path="MethodValidity.html"><a href="MethodValidity.html#confidence-interval-calibration-1"><i class="fa fa-check"></i><b>19.3.5</b> Confidence interval calibration</a></li>
<li class="chapter" data-level="19.3.6" data-path="MethodValidity.html"><a href="MethodValidity.html#between-database-heterogeneity"><i class="fa fa-check"></i><b>19.3.6</b> Between-database heterogeneity</a></li>
<li class="chapter" data-level="19.3.7" data-path="MethodValidity.html"><a href="MethodValidity.html#sensitivity-analyses-1"><i class="fa fa-check"></i><b>19.3.7</b> Sensitivity analyses</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="MethodValidity.html"><a href="MethodValidity.html#ohdsi-methods-benchmark"><i class="fa fa-check"></i><b>19.4</b> OHDSI Methods Benchmark</a></li>
<li class="chapter" data-level="19.5" data-path="MethodValidity.html"><a href="MethodValidity.html#summary-5"><i class="fa fa-check"></i><b>19.5</b> Summary</a></li>
<li class="chapter" data-level="19.6" data-path="MethodValidity.html"><a href="MethodValidity.html#exercises-3"><i class="fa fa-check"></i><b>19.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V OHDSI Studies</b></span></li>
<li class="chapter" data-level="20" data-path="StudySteps.html"><a href="StudySteps.html"><i class="fa fa-check"></i><b>20</b> Study steps</a></li>
<li class="chapter" data-level="21" data-path="NetworkResearch.html"><a href="NetworkResearch.html"><i class="fa fa-check"></i><b>21</b> OHDSI Network Research</a><ul>
<li class="chapter" data-level="21.1" data-path="NetworkResearch.html"><a href="NetworkResearch.html#ohdsi-network-study-examples"><i class="fa fa-check"></i><b>21.1</b> OHDSI Network Study Examples</a><ul>
<li class="chapter" data-level="21.1.1" data-path="NetworkResearch.html"><a href="NetworkResearch.html#endometriosis-study"><i class="fa fa-check"></i><b>21.1.1</b> Endometriosis study</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="NetworkResearch.html"><a href="NetworkResearch.html#excercises-2"><i class="fa fa-check"></i><b>21.2</b> Excercises</a><ul>
<li class="chapter" data-level="21.2.1" data-path="NetworkResearch.html"><a href="NetworkResearch.html#defining-a-cohort"><i class="fa fa-check"></i><b>21.2.1</b> Defining a cohort</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="Glossary.html"><a href="Glossary.html"><i class="fa fa-check"></i><b>A</b> Glossary</a></li>
<li class="chapter" data-level="B" data-path="CohortDefinitions.html"><a href="CohortDefinitions.html"><i class="fa fa-check"></i><b>B</b> Cohort definitions</a><ul>
<li class="chapter" data-level="B.1" data-path="CohortDefinitions.html"><a href="CohortDefinitions.html#AceInhibitors"><i class="fa fa-check"></i><b>B.1</b> ACE inhibitors</a></li>
<li class="chapter" data-level="B.2" data-path="CohortDefinitions.html"><a href="CohortDefinitions.html#AceInhibitorsMono"><i class="fa fa-check"></i><b>B.2</b> New users of ACE inhibitors as first-line monotherapy for hypertension</a></li>
<li class="chapter" data-level="B.3" data-path="CohortDefinitions.html"><a href="CohortDefinitions.html#Ami"><i class="fa fa-check"></i><b>B.3</b> Acute myocardial infarction (AMI)</a></li>
<li class="chapter" data-level="B.4" data-path="CohortDefinitions.html"><a href="CohortDefinitions.html#Angioedema"><i class="fa fa-check"></i><b>B.4</b> Angioedema</a></li>
<li class="chapter" data-level="B.5" data-path="CohortDefinitions.html"><a href="CohortDefinitions.html#ThiazidesMono"><i class="fa fa-check"></i><b>B.5</b> New users of Thiazide-like diuretics as first-line monotherapy for hypertension</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="NegativeControlsAppendix.html"><a href="NegativeControlsAppendix.html"><i class="fa fa-check"></i><b>C</b> Negative controls</a><ul>
<li class="chapter" data-level="C.1" data-path="NegativeControlsAppendix.html"><a href="NegativeControlsAppendix.html#AceiThzNsc"><i class="fa fa-check"></i><b>C.1</b> ACEi and THZ</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Book of OHDSI</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="MethodValidity" class="section level1">
<h1><span class="header-section-number">Chapter 19</span> Method Validity</h1>
<p><em>Chapter lead: Martijn Schuemie</em></p>
<p>When considering method validity we aim to answer the question</p>
<blockquote>
<p>Is this method valid for answering this question?</p>
</blockquote>
<p>Where “method” includes not only the study design, but als the data and the implementation of the design. Method validity is therefore somewhat of a catch-all; It is often not possible to observe good method validity without good data quality, clinical validity, and software validity. Those aspects of evidence quality should have already been addressed separately before we consider method validity.</p>
<p>The core activity when establishing method validity is evaluating whether important assumptions in the analysis have been met. For example, we assume that propensity-score matching makes two populations comparable, but we need to evaluate whether this is the case. Where possible, empirical tests should be performed to verify these assumptions. We can for example generate diagnostics to show that our two populations are indeed comparable on a wide range of characteristics after matching. In OHDSI we have developed a wide range of standardized diagnostics that should be generated and evaluated whenever an analysis is performed.</p>
<p>In this chapter we will focus on the validity of methods use in population-level estimation. We will first briefly highlight some study design-specific diagnostics, and will then discuss diagnostics that are applicable to most if not all population-level estimation studies. Following this is a step-by-step description of how to execute these diagnostics using the OHDSI tools. We close this chapter with an advanced topic, reviewing the OHDSI Methods Benchmark and its application to the OHDSI Methods Library.</p>
<div id="design-specific-diagnostics" class="section level2">
<h2><span class="header-section-number">19.1</span> Design-specific diagnostics</h2>
<p>For each study design there are diagnostics specific to such a design. Many of these diagnostics are implemented and readily available in the R packages of the <a href="https://ohdsi.github.io/MethodsLibrary/">OHDSI Methods Library</a>. For example, Section <a href="PopulationLevelEstimation.html#studyOutputs">13.9</a> lists a wide range of diagnostics generated by the <a href="https://ohdsi.github.io/CohortMethod/">CohortMethod</a> package, including:</p>
<ul>
<li><strong>Propensity score distribution</strong> to asses initial comparability of cohorts.</li>
<li><strong>Propensity model</strong> to identify potential variables that should be excluded from the model.</li>
<li><strong>Covariate balance</strong> to evaluate whether propensity score adjustment has made the cohorts comparable (as measured through baseline covariates).</li>
<li><strong>Attrition</strong> to observe how many subjects were excluded, which may inform on the generalizability of the results to the initial cohorts of interest.</li>
<li><strong>Power</strong> to assess whether enough data is available to answer the question.</li>
<li><strong>Kaplan Meier curve</strong> to asses typical time to onset, and whether the proportionality assumption underlying Cox models is met.</li>
</ul>
<p>Other study designs require different diagnostics to test the different assumptions in those designs. For example, for the self-controlled case series (SCCS) design we may check the necessary assumption that the end of observation is independent of the outcome. This assumption is often violated in the case of serious, potentially lethal, events such as myocardial infarction. We can evaluate whether the assumption holds by generating the plot shown in Figure <a href="MethodValidity.html#fig:timeToObsEnd">19.1</a>, which shows a histograms of the time to obsevation period end for those that are censored, and those that uncensored. In our data we consider those whose observation period ends at the end date of data capture (the date when observation stopped for the entire data base, for example the date of extraction, or the study end date) to be uncensored, and all others to be censored. In Figure <a href="MethodValidity.html#fig:timeToObsEnd">19.1</a> we see only minor differences between the two distributions, suggesting our assumptions holds.</p>
<div class="figure" style="text-align: center"><span id="fig:timeToObsEnd"></span>
<img src="images/MethodValidity/timeToObsEnd.png" alt="Time to observation end for those that are censored, and those that uncensored." width="100%" />
<p class="caption">
Figure 19.1: Time to observation end for those that are censored, and those that uncensored.
</p>
</div>
</div>
<div id="diagnostics-for-all-estimation" class="section level2">
<h2><span class="header-section-number">19.2</span> Diagnostics for all estimation</h2>
<p>Next to the design-specific diagnostics, there are also several diagnostics that are applicable across all causal effect estimation methods. Many of these rely on the use of control hypotheses, research questions where the answer is already known. Using control hypotheses we can then evaluate whether our design produces results in line with the truth. Controls can be divided into negative controls and positive controls.</p>
<div id="NegativeControls" class="section level3">
<h3><span class="header-section-number">19.2.1</span> Negative controls</h3>
<p>Negative controls are exposure-outcome pairs where one believes no causal effect exists, and including negative controls or “falsification endpoints” <span class="citation">(Prasad and Jena <a href="#ref-prased_2013">2013</a>)</span> has been recommended as a means to detect confounding, <span class="citation">(Lipsitch, Tchetgen Tchetgen, and Cohen <a href="#ref-lipsitch_2010">2010</a>)</span> selection bias and measurement error. <span class="citation">(Arnold et al. <a href="#ref-arnold_2016">2016</a>)</span> For example, in one study <span class="citation">(Zaadstra et al. <a href="#ref-zaadstra_2008">2008</a>)</span> investigating the relationship between childhood diseases and later multiple sclerosis (MS), the authors include three negative controls that are not believed to cause MS: a broken arm, concussion, and tonsillectomy. Two of these three controls produce statistically significant associations with MS, suggesting that the study may be biased.</p>
<p>We should select negative controls that are comparable to our hypothesis of interest, which means we typically select exposure-outcome pairs that either have the same exposure as the hypothesis of interest (so-called “outcome controls”) or the same outcome (“exposure controls”). Our negative controls should further meet these criteria:</p>
<ul>
<li>The exposure <strong>cannot cause</strong> the outcome. One way to think of causation is to think of the counterfactual: could the outcome be caused (or prevented) if a patient was not exposed, compared to if the patient had been exposed? Sometimes this is clear, for example ACEi are known to cause angioedema. Other times this is far less obvious. For example, a drug that may cause hypertension can therefore indirectly cause cardiovascular diseases that are a consequence of the hypertension.</li>
<li>The exposure should also <strong>not prevent or treat</strong> the outcome. This is just another causal relationship that should be absent if we are to believe the true effect size (e.g. the hazard ratio) is 1.</li>
<li>The negative control should <strong>exist in the data</strong>, ideally with sufficient numbers. We try to achieve this by rank-ordering the candidate list by prevalence.</li>
<li>Negative controls should ideally be <strong>independent</strong>. For example, we should avoid having negative controls that are either ancestors of each other (e.g. “ingrown nail” and “ingrown nail of foot”) or siblings (e.g. “fracture of left femur” and “fracture of right femur”).</li>
<li>Negative controls should ideally have <strong>some potential for bias</strong>. For example, the last digit of someone’s social security number is basically a random number, and is unlikely to show confounding. It should therefore not be used as a negative control.</li>
</ul>
<p>The absence of a causal relationship between an exposure and an outcome is rarely documented. Instead, we often make the assumption that a lack of evidence of a relationship implies the lack of a relationship. This assumption is more likely to hold if the exposure and outcome have both been studies extensively, so a relationship could have been detected. For example, the lack of evidence for a completely novel drug likely implies a lack of knowledge, not the lack of a relationship. With this principe in mind we have developed a semi-automated procedure for selecting negative controls <span class="citation">(Voss et al. <a href="#ref-voss_2016">2016</a>)</span>. In brief, information from literature, product labels, and spontaneous reporting is automatically extracted and synthesized to produce a candidate list of negative controls. This list must then undergo manual review, not only to verify that the automated extraction was accurate, but also to impose additional criteria such as biological plausibility.</p>
</div>
<div id="PositiveControls" class="section level3">
<h3><span class="header-section-number">19.2.2</span> Positive controls</h3>
<p>To understand the behavior of a method when the true relative risk is smaller or greater than one requires the use of positive controls, where the null is believed to not be true. Unfortunately, real positive controls for observational research tend to be problematic for three reasons. First, in most research contexts, for example when comparing the effect of two treatments, there is a paucity of positive controls relevant for that specific context. Second, even if positive controls are available, the magnitude of the effect size may not be known with great accuracy, and often depends on the population in which one measures it. Third, when treatments are widely known to cause a particular outcome, this shapes the behavior of physicians prescribing the treatment, for example by taking actions to mitigate the risk of unwanted outcomes, thereby rendering the positive controls useless as a means for evaluation. <span class="citation">(Noren et al. <a href="#ref-noren_2014">2014</a>)</span></p>
<p>In OHDSI we therefore use synthetic positive controls, <span class="citation">(Schuemie, Hripcsak, et al. <a href="#ref-schuemie_2018">2018</a>)</span> created by modifying a negative control through injection of additional, simulated occurrences of the outcome during the time at risk of the exposure. One issue that stands important is the preservation of confounding. The negative controls may show strong confounding, but if we inject additional outcomes randomly, these new outcomes will not be confounded, and we may therefore be optimistic in our evaluation of our capacity to deal with confounding for positive controls. To preserve confounding, we want the new outcomes to show similar associations with baseline subject-specific covariates as the original outcomes. To achieve this, for each outcome we train a model to predict the survival rate with respect to the outcome during exposure using covariates captured prior to exposure. These covariates include demographics, as well as all recorded diagnoses, drug exposures, measurements, and medical procedures. An L1-regularized Poisson regression <span class="citation">(Suchard et al. <a href="#ref-suchard_2013">2013</a>)</span> using 10-fold cross-validation to select the regularization hyperparameter fits the prediction model. We then use the predicted rates to sample simulated outcomes during exposure to increase the true effect size to the desired magnitude. The resulting positive control thus contains both real and simulated outcomes. Figure <a href="MethodValidity.html#fig:posControlSynth">19.2</a> depicts this process. Note that although this procedure simulates several important sources of bias, it does not capture all. For example, some effects of measurement error are not present. The synthetic positive controls imply constant positive predictive value and sensitivity, which may not be true in reality.</p>
<div class="figure" style="text-align: center"><span id="fig:posControlSynth"></span>
<img src="images/MethodValidity/posControlSynth.png" alt="Synthesizing positive controls from negative controls." width="90%" />
<p class="caption">
Figure 19.2: Synthesizing positive controls from negative controls.
</p>
</div>
<p>Although we refer to a single true “effect size” for each control, different methods estimate different statistics of the treatment effect. For negative controls, where we believe no causal effect exists, all such statistics, including the relative risk, hazard ratio, odds ratio, incidence rate ratio, both conditional and marginal, as well as the average treatment effect in the treated (ATT) and the overall average treatment effect (ATE) will be identical to 1. Our process for creating positive controls synthesizes outcomes with a constant incidence rate ratio over time and between patients, using a model conditioned on the patient where this ratio is held constant, up to the point where the marginal effect is achieved. The true effect size is thus guaranteed to hold as the marginal incidence rate ratio in the treated. Under the assumption that our outcome model used during synthesis is correct, this also holds for the conditional effect size and the ATE. Since all outcomes are rare, odds ratios are all but identical to the relative risk.</p>
</div>
<div id="metrics" class="section level3">
<h3><span class="header-section-number">19.2.3</span> Empirical evaluation</h3>
<p>Based on the estimates of a particular method for the negative and positive controls, we can then understand the operating characteristic by computing a range of metrics, for example:</p>
<ul>
<li><strong>Area Under the receiver operator Curve (AUC)</strong>: the ability to discriminate between positive and negative controls.</li>
<li><strong>Coverage</strong>: how often the true effect size is within the 95% confidence interval.</li>
<li><strong>Mean precision</strong>: precision is computed as <span class="math inline">\(1/(standard\ error)^2\)</span>, higher precision means narrower confidence intervals. We use the geometric mean to account for the skewed distribution of the precision.</li>
<li><strong>Mean squared error (MSE)</strong>: Mean squared error between the log of the effect size point-estimate and the log of the true effect size.</li>
<li><strong>Type 1 error</strong>: For negative controls, how often was the null rejected (at <span class="math inline">\(\alpha = 0.05\)</span>). This is equivalent to the false positive rate and <span class="math inline">\(1 - specificity\)</span>.</li>
<li><strong>Type 2 error</strong>: For positive controls, how often was the null not rejected (at <span class="math inline">\(\alpha = 0.05\)</span>). This is equivalent to the false negative rate and <span class="math inline">\(1 - sensitivity\)</span>.</li>
<li><strong>Non-estimable</strong>: For how many of the controls was the method unable to produce an estimate? There can be various reasons why an estimate cannot be produced, for example because there were no subjects left after propensity score matching, or because no subjects remained having the outcome.</li>
</ul>
<p>Depending on our use case, we can evaluate whether these operating characterists are suitable for our goal. For example, if we wish to perform signal detection, we may care about type I and type II error, or if we are willing to modify our <span class="math inline">\(\alpha\)</span> threshold, we may inspect the AUC instead.</p>
</div>
<div id="p-value-calibration" class="section level3">
<h3><span class="header-section-number">19.2.4</span> P-value calibration</h3>
<p>Often the type I error (at <span class="math inline">\(\alpha = 0.05\)</span>) is larger than 5%. In other words, we are often more likely than 5% to reject the null hypothesis when in fact the null hypothesis is true. The reason is that the p-value only reflects random error, the error due to having a limited sample size. It does not reflect systematic error, for example the error due to confounding. OHDSI has developed a process for calibrating p-values to restore the type I error to nominal. <span class="citation">(Schuemie et al. <a href="#ref-schuemie_2014">2014</a>)</span> We derive an empirical null distribution from the actual effect estimates for the negative controls. These negative control estimates give us an indication of what can be expected when the null hypothesis is true, and we use them to estimate an empirical null distribution.</p>
<p>Formally, we fit a Gaussian probability distribution to the estimates, taking into account the sampling error of each estimate. Let <span class="math inline">\(\hat{\theta}_i\)</span> denote the estimated log effect estimate (relative risk, odds or incidence rate ratio) from the <span class="math inline">\(i\)</span>th negative control drug–outcome pair, and let <span class="math inline">\(\hat{\tau}_i\)</span> denote the corresponding estimated standard error, <span class="math inline">\(i=1,\ldots,n\)</span>. Let <span class="math inline">\(\theta_i\)</span> denote the true log effect size (assumed 0 for negative controls), and let <span class="math inline">\(\beta_i\)</span> denote the true (but unknown) bias associated with pair <span class="math inline">\(i\)</span> , that is, the difference between the log of the true effect size and the log of the estimate that the study would have returned for control <span class="math inline">\(i\)</span> had it been infinitely large. As in the standard p-value computation, we assume that <span class="math inline">\(\hat{\theta}_i\)</span> is normally distributed with mean <span class="math inline">\(\theta_i + \beta_i\)</span> and standard deviation <span class="math inline">\(\hat{\tau}_i^2\)</span>. Note that in traditional p-value calculation, <span class="math inline">\(\beta_i\)</span> is always assumed to be equal to zero, but that we assume the <span class="math inline">\(\beta_i\)</span>’s, arise from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This represents the null (bias) distribution. We estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> via maximum likelihood. In summary, we assume the following: <span class="math display">\[\beta_i \sim N(\mu,\sigma^2) \text{  and} \\ \hat{\theta}_i \sim N(\theta_i + \beta_i, \tau_i^2)\]</span></p>
<p>where <span class="math inline">\(N(a,b)\)</span> denotes a Gaussian distribution with mean <span class="math inline">\(a\)</span> and variance <span class="math inline">\(b\)</span>, and estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> by maximizing the following likelihood: <span class="math display">\[L(\mu, \sigma | \theta, \tau) \propto \prod_{i=1}^{n}\int p(\hat{\theta}_i|\beta_i, \theta_i, \hat{\tau}_i)p(\beta_i|\mu, \sigma) \text{d}\beta_i\]</span></p>
<p>yielding maximum likelihood estimates <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>. We compute a calibrated p-value that uses the empirical null distribution. Let <span class="math inline">\(\hat{\theta}_{n+1}\)</span> denote the log of the effect estimate from a new drug–outcome pair, and let <span class="math inline">\(\hat{\tau}_{n+1}\)</span> denote the corresponding estimated standard error. From the aforementioned assumptions and assuming <span class="math inline">\(\beta_{n+1}\)</span> arises from the same null distribution, we have the following:</p>
<p><span class="math display">\[\hat{\theta}_{n+1} \sim N(\hat{\mu}, \hat{\sigma} + \hat{\tau}_{n+1})\]</span></p>
<p>When <span class="math inline">\(\hat{\theta}_{n+1}\)</span> is smaller than <span class="math inline">\(\hat{\mu}\)</span>, the one-sided calibrated p-value for the new pair is then</p>
<p><span class="math display">\[\phi\left(\frac{\theta_{n+1} - \hat{\mu}}{\sqrt{\hat{\sigma}^2 + \hat{\tau}_{n+1}^2}}\right)\]</span></p>
<p>where <span class="math inline">\(\phi(\cdot)\)</span> denotes the cumulative distribution function of the standard normal distribution. When <span class="math inline">\(\hat{\theta}_{n+1}\)</span> is bigger than <span class="math inline">\(\hat{\mu}\)</span>, the one-sided calibrated p-value is then <span class="math display">\[1-\phi\left(\frac{\theta_{n+1} - \hat{\mu}}{\sqrt{\hat{\sigma}^2 + \hat{\tau}_{n+1}^2}}\right)\]</span></p>
</div>
<div id="confidence-interval-calibration" class="section level3">
<h3><span class="header-section-number">19.2.5</span> Confidence interval calibration</h3>
<p>Similarly, we typically observe that the coverage of the 95% confidence interval is less than 95%: the true effect size is inside the 95% confidence interval less than 95% of the time. For confidence interval calibration <span class="citation">(Schuemie, Hripcsak, et al. <a href="#ref-schuemie_2018">2018</a>)</span> we extend the framework for p-value calibration by also making use of our positive controls. Typically, but not necessarily, the calibrated confidence interval is wider than the nominal confidence interval, reflecting the problems unaccounted for in the standard procedure (such as unmeasured confounding, selection bias and measurement error) but accounted for in the calibration.</p>
<p>Formally, we assume that <span class="math inline">\(beta_i\)</span>, the bias associated with pair <span class="math inline">\(i\)</span>, again comes from a Gaussian distribution, but this time using a mean and standard deviation that are linearly related to <span class="math inline">\(theta_i\)</span>, the true effect size:</p>
<p><span class="math display">\[\beta_i \sim N(\mu(\theta_i) , \sigma^2(\theta_i))\]</span></p>
<p>where</p>
<p><span class="math display">\[\mu(\theta_i) = a + b \times \theta_i \text{ and} \\
  \sigma(\theta_i) ^2= c + d \times \mid \theta_i \mid\]</span></p>
<p>We estimate <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> by maximizing the marginalized likelihood in which we integrate out the unobserved <span class="math inline">\(\beta_i\)</span>:</p>
<p><span class="math display">\[l(a,b,c,d | \theta, \hat{\theta}, \hat{\tau} ) \propto \prod_{i=1}^{n}\int p(\hat{\theta}_i|\beta_i, \theta_i, \hat{\tau}_i)p(\beta_i|a,b,c,d,\theta_i) \text{d}\beta_i ,\]</span> yielding maximum likelihood estimates <span class="math inline">\((\hat{a}, \hat{b}, \hat{c}, \hat{d})\)</span>.</p>
<p>We compute a calibrated CI that uses the systematic error model. Let <span class="math inline">\(\hat{\theta}_{n+1}\)</span> again denote the log of the effect estimate for a new outcome of interest, and let <span class="math inline">\(\hat{\tau}_{n+1}\)</span> denote the corresponding estimated standard error. From the assumptions above, and assuming <span class="math inline">\(\beta_{n+1}\)</span> arises from the same systematic error model, we have:</p>
<p><span class="math display">\[\hat{\theta}_{n+1} \sim N(
\theta_{n+1} + \hat{a} + \hat{b} \times \theta_{n+1},
\hat{c} + \hat{d} \times \mid \theta_{n+1} \mid) + \hat{\tau}_{n+1}^2) .\]</span></p>
<p>We find the lower bound of the calibrated 95% CI by solving this equation for <span class="math inline">\(\theta_{n+1}\)</span>:</p>
<p><span class="math display">\[\Phi\left(
\frac{\theta_{n+1} + \hat{a} + \hat{b} \times \theta_{n+1}-\hat{\theta}_{n+1}}
{\sqrt{(\hat{c} + \hat{d} \times \mid \theta_{n+1} \mid) + \hat{\tau}_{n+1}^2}}
\right) = 0.025 ,\]</span></p>
<p>where <span class="math inline">\(\Phi(\cdot)\)</span> denotes the cumulative distribution function of the standard normal distribution. We find the upper bound similarly for probability 0.975. We define the calibrated point estimate by using probability 0.5.</p>
<p>Both p-value calibration and confidence interval calibration are implemented in the <a href="https://ohdsi.github.io/EmpiricalCalibration/">EmpiricalCalibration</a> package.</p>
</div>
<div id="replication-across-sites" class="section level3">
<h3><span class="header-section-number">19.2.6</span> Replication across sites</h3>
<p>Another form of method validation comes from executing the study across several different databases that represent different populations, different health care systems, and/or different data capture processes. Prior research has shown that executing the same study design across different databases can produce vastly different effect size estimates, <span class="citation">(Madigan et al. <a href="#ref-madigan_2013">2013</a>)</span> suggesting that either the effect differs greatly for different populations, or that the design does not adequately address the different biases found in the different databases. In fact, we observe that accounting for residual bias in a database through empirical calibration of confidence intervals can greatly reduce between-study heterogeneity. <span class="citation">(Schuemie, Hripcsak, et al. <a href="#ref-schuemie_2018">2018</a>)</span></p>
<p>One way to express between-database heterogeneity is the <span class="math inline">\(I^2\)</span> score, describing the percentage of total variation across studies that is due to heterogeneity rather than chance. <span class="citation">(Higgins et al. <a href="#ref-higgins_2003">2003</a>)</span> A naive categorisation of values for <span class="math inline">\(I^2\)</span> would not be appropriate for all circumstances, although one could tentatively assign adjectives of low, moderate, and high to <span class="math inline">\(I^2\)</span> values of 25%, 50%, and 75%. In a study estimation many effects for depression treatments using a new-user cohort design with large-scale propensity score adjustment, <span class="citation">Schuemie, Ryan, et al. (<a href="#ref-schuemie_2018b">2018</a>)</span> observed only 58% of the estimates to have an <span class="math inline">\(I^2\)</span> below 25%. After empirical calibration this increased to 83%.</p>

<div class="rmdimportant">
Observing between-database heterogeneity casts doubt on the validity of the estimates. Unfortunately, the inverse is not true. Not observing heterogeneity does not guarantee an unbiased estimate. It is not unlikely that all databases share a similar bias, and that all estimates are therefore consistently wrong.
</div>

</div>
<div id="sensitivity-analyses" class="section level3">
<h3><span class="header-section-number">19.2.7</span> Sensitivity analyses</h3>
<p>When designing a study there are often design choices that are uncertain. For example, should propensity score matching of stratification be used? If stratification is used, how many strata? What is the appropriate time-at-risk? When faced with such uncertainty, one solution is to evaluate various options, and observe the sensitivity of the results to the design choice. If the estimate remains the same under various options, we can say the study is robust to the uncertainty.</p>
<p>This definition of sensitivity analysis should not be confused with the definitions used by others such as <span class="citation">Rosenbaum (<a href="#ref-rosenbaum_2005">2005</a>)</span>, who define sensitivity analysis to “appraise how the conclusions of a study might be altered by hidden biases of various magnitudes”.</p>
</div>
</div>
<div id="method-validation-in-practice" class="section level2">
<h2><span class="header-section-number">19.3</span> Method validation in practice</h2>
<p>Here we build on the example in Chapter <a href="PopulationLevelEstimation.html#PopulationLevelEstimation">13</a>, where we investigate the effect of ACE inhibitors (ACEi) on the risk of angioedema and acute myocardial infarction (AMI), compared to thiazides and thiazide-like diuretics (THZ). In that chapter we already explore many of the diagnostics specific to the design we used: the cohort method. Here, we apply additional diagnostics that could also have been applied had other designs been used. If the study is implemented using ATLAS as described in Section <a href="PopulationLevelEstimation.html#PleAtlas">13.7</a> these diagnostics are available in the Shiny app that is included in the study R package generated by ATLAS. If the study is implemented using R instead, as described in Section <a href="PopulationLevelEstimation.html#pleR">13.8</a>, then R functions available in the various packages should be used, as described in the next sections.</p>
<div id="selecting-negative-controls" class="section level3">
<h3><span class="header-section-number">19.3.1</span> Selecting negative controls</h3>
<p>We must select negative controls, exposure-outcome pairs where no causal effect is believed to exist. For comparative effect estimation such as our example study, we select negative control outcomes that are believed to be neither caused by the target nor the comparator exposure. We want enough negative controls to make sure we have a diverse mix of biases represented in the controls, and also to allow empirical calibration. As a rule-of-thumb we typically aim to have 50-100 such negative controls. We could come up with these controls completely manually, but fortunately ATLAS provides features to aid the selection of negative controls using data from literature, product labels, and spontaneous reports.</p>
<p>To generate a canidate list of negative controls, we first must create a concept set containing all exposures of interest. In this case we select all ingredients in the ACEi and THZ classes, as shown in Figure <a href="MethodValidity.html#fig:exposuresConceptSet">19.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:exposuresConceptSet"></span>
<img src="images/MethodValidity/exposuresConceptSet.png" alt="A concept set containing the concepts defining the target and comparator exposures." width="100%" />
<p class="caption">
Figure 19.3: A concept set containing the concepts defining the target and comparator exposures.
</p>
</div>
<p>Next, we go to the “Explore Evidence” tab, and click on the <img src="images/MethodValidity/generate.png" /> button. Generating the evidence overview will take a few minutes, after which you can click on the <img src="images/MethodValidity/viewEvidence.png" /> button. This will open the list of outcomes as shown in Figure <a href="MethodValidity.html#fig:candidateNcs">19.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:candidateNcs"></span>
<img src="images/MethodValidity/candidateNcs.png" alt="Candidate control outcomes with an overview of the evidence found in literature, product labels, and spontaneous reports." width="100%" />
<p class="caption">
Figure 19.4: Candidate control outcomes with an overview of the evidence found in literature, product labels, and spontaneous reports.
</p>
</div>
<p>This list shows condition concepts, along with an overview of the evidence linking the condition to any of the exposures we defined. For example, we see the number of publications that link the exposures to the outcomes found in PubMed using various strategies, the number of product labels of our exposures of interest that list the condition as a possible adverse effect, and the number of spontaneous reports. By default the list is sorted to show candidate negative controls first. It is then sorted by the “Sort Order”, which represents the prevalance of the condition in a collection of observational databases. The higher the Sort Order, the higher the prevalence. Although the prevalence in these databases might not correspond with the prevalence in the database we wish to run the study, it is likely a good approximation.</p>
<p>The next step is to manually review the candidate list, typically starting at the top, so with the most prevalent condition, and working our way down until we are satisfied we have enough. One typical way to do this is to export the list to CSV, and have clinicians review these, considering the criteria mentioned in Section <a href="MethodValidity.html#NegativeControls">19.2.1</a>.</p>
<p>For our example study we select the 76 negative controls listed in Appendix <a href="NegativeControlsAppendix.html#AceiThzNsc">C.1</a>.</p>
</div>
<div id="including-controls" class="section level3">
<h3><span class="header-section-number">19.3.2</span> Including controls</h3>
<p>Once we have defined our set of negative controls we must include them in our study. First we must define some logic for turning our negative control condition concepts into outcome cohorts. Section <a href="PopulationLevelEstimation.html#evaluationSettings">13.7.3</a> discusses how ATLAS allows creating such cohorts based on a few choices the user must make. Often we simply choose to create a cohort based on any occurrence of a negative control concept or any of its descendants. If the study is implemented in R then SQL (Structured Query Language) can be used to construct the negative control cohorts. Chapter <a href="SqlAndR.html#SqlAndR">10</a> describes how cohorts can be created using SQL and R. We leave it as an exercise for the reader to write the appropriate SQL and R.</p>
<p>The OHDSI tools also provide functionality for automatically generating and including positive controls derived from the negative controls. This functionality can be found in the Evaluation Settings section in ATLAS described in Section <a href="PopulationLevelEstimation.html#evaluationSettings">13.7.3</a>, and is implemented in the <code>injectSignals</code> function in the <a href="https://ohdsi.github.io/MethodEvaluation/">MethodEvaluation</a> package. Here we generate three positive controls for each negative control, with true effect sizes of 1.5, 2, and 4, using a survival model.</p>
<p>Next we must execute the same study used to estimate the effect of interest to also estimate effects for the negative and positive controls. Setting the set of negative controls in the comparisons dialog in ATLAS instructs ATLAS to compute estimates for these controls. Similarly, specifying that positive controls be generated in the Evaluation Settings includes these in our analysis. In R, the negative and positive controls should be treated as any other outcome. All estimation packages in the <a href="https://ohdsi.github.io/MethodsLibrary/">OHDSI Methods Library</a> readily allow estimation of many effects in an efficient manner.</p>
</div>
<div id="empirical-performance" class="section level3">
<h3><span class="header-section-number">19.3.3</span> Empirical performance</h3>
<p>Figure <a href="MethodValidity.html#fig:controls">19.5</a> shows the estimated effect sizes for the negative and positive controls included in our example study, stratified by true effect size. This plot is included in the Shiny app that comes with the study R package generated by ATLAS, and can be generated using the <code>plotControls</code> function in the <a href="https://ohdsi.github.io/MethodEvaluation/">MethodEvaluation</a> package. Note that the number of controls is often lower than what was defined because there was not enough data to either produce an estimate, or to synthesize a positive control.</p>
<div class="figure" style="text-align: center"><span id="fig:controls"></span>
<img src="images/MethodValidity/controls.png" alt="Estimates for the negative (true hazard ratio = 1) and positive controls (true hazard ratio &gt; 1). Each dot represents a control. Estimates below the dashed line have a confidence interval that doesn't include the true effect size." width="100%" />
<p class="caption">
Figure 19.5: Estimates for the negative (true hazard ratio = 1) and positive controls (true hazard ratio &gt; 1). Each dot represents a control. Estimates below the dashed line have a confidence interval that doesn’t include the true effect size.
</p>
</div>
<p>Based on these estimates we can compute the metrics shown in Table <a href="MethodValidity.html#tab:exampleMetrics">19.1</a> using the <code>computeMetrics</code> function in the <a href="https://ohdsi.github.io/MethodEvaluation/">MethodEvaluation</a> package.</p>
<table>
<caption><span id="tab:exampleMetrics">Table 19.1: </span> Method performance metrics derived from the negative and positive control estimates.</caption>
<thead>
<tr class="header">
<th align="left">Metric</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">AUC</td>
<td align="right">0.96</td>
</tr>
<tr class="even">
<td align="left">Coverage</td>
<td align="right">0.97</td>
</tr>
<tr class="odd">
<td align="left">Mean Precision</td>
<td align="right">19.33</td>
</tr>
<tr class="even">
<td align="left">MSE</td>
<td align="right">2.08</td>
</tr>
<tr class="odd">
<td align="left">Type 1 error</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="left">Type 2 error</td>
<td align="right">0.18</td>
</tr>
<tr class="odd">
<td align="left">Non-estimable</td>
<td align="right">0.08</td>
</tr>
</tbody>
</table>
<p>We see that coverage and type 1 error are very close to their nominal values of 95% and 5%, respectively, and that the AUC is very high. This is certainly not always the case.</p>
<p>Note that although in Figure <a href="MethodValidity.html#fig:controls">19.5</a> not all confidence intervals include one when the true hazard ratio is one, the type 1 error in Table <a href="MethodValidity.html#tab:exampleMetrics">19.1</a> is 0%. This is an exceptional situation, caused by the fact that confidence intervals in the <a href="https://ohdsi.github.io/Cyclops/">Cyclops</a> package are estimated using likelihood profiling, which is more accurate than traditional methods but can result in assymmetric confidence intervals. The p-value instead is computed assuming symmetrical confidence intervals, and this is what was used to compute the type 1 error.</p>
</div>
<div id="p-value-calibration-1" class="section level3">
<h3><span class="header-section-number">19.3.4</span> P-value calibration</h3>
<p>We can use the estimates for our negative controls to calibrate our p-values. This is done automatically in the Shiny app, and can be done manually in R. Assuming we have created the summary object <code>summ</code> as described in Section <a href="PopulationLevelEstimation.html#MultipleAnalyses">13.8.6</a>, we can plot the empirical calibration effect plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimates for negative controls (ncs) and outcomes of interest (ois):</span>
ncEstimates &lt;-<span class="st"> </span>summ[summ<span class="op">$</span>outcomeId <span class="op">%in%</span><span class="st"> </span>ncs, ]
oiEstimates &lt;-<span class="st"> </span>summ[summ<span class="op">$</span>outcomeId <span class="op">%in%</span><span class="st"> </span>ois, ]

<span class="kw">library</span>(EmpiricalCalibration)
<span class="kw">plotCalibrationEffect</span>(<span class="dt">logRrNegatives =</span> ncEstimates<span class="op">$</span>logRr,
                      <span class="dt">seLogRrNegatives =</span> ncEstimates<span class="op">$</span>seLogRr,
                      <span class="dt">logRrPositives =</span> oiEstimates<span class="op">$</span>logRr,
                      <span class="dt">seLogRrPositives =</span> oiEstimates<span class="op">$</span>seLogRr,
                      <span class="dt">showCis =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:pValueCal"></span>
<img src="images/MethodValidity/pValueCal.png" alt="P-value calibration: estimates below the dashed line have a conventional p &lt; 0.05. Estimates in the orange area have calibrated p &lt; 0.05. The pink area denotes the 95% credible interval around the edge of the orange area. Blue dots indicate negative controls. Yellow diamonds indicate outcomes of interest." width="70%" />
<p class="caption">
Figure 19.6: P-value calibration: estimates below the dashed line have a conventional p &lt; 0.05. Estimates in the orange area have calibrated p &lt; 0.05. The pink area denotes the 95% credible interval around the edge of the orange area. Blue dots indicate negative controls. Yellow diamonds indicate outcomes of interest.
</p>
</div>
<p>In Figure <a href="MethodValidity.html#fig:pValueCal">19.6</a> we see that the orange area almost exactly overlaps with the area denoted by the dashed lines, indicated hardly any bias was observed for the negative controls. One of the outcomes of interest (AMI) is above the dashed line and the orange area, indicating we cannot reject the null according to both the uncalibrated and calibrated p-value. The other outcome (angioedema) clearly stands out from the negative control, and falls well within the area where both uncalibrated and calibrated p-values are smaller than 0.05.</p>
<p>We can compute the calibrated p-values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">null &lt;-<span class="st"> </span><span class="kw">fitNull</span>(<span class="dt">logRr =</span> ncEstimates<span class="op">$</span>logRr,
                <span class="dt">seLogRr =</span> ncEstimates<span class="op">$</span>seLogRr)
<span class="kw">calibrateP</span>(null,
           <span class="dt">logRr=</span> oiEstimates<span class="op">$</span>logRr,
           <span class="dt">seLogRr =</span> oiEstimates<span class="op">$</span>seLogRr)</code></pre></div>
<pre><code>## [1] 1.604351e-06 7.159506e-01</code></pre>
<p>And contrast these with the uncalibrated p-values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oiEstimates<span class="op">$</span>p</code></pre></div>
<pre><code>## [1] [1] 1.483652e-06 7.052822e-01</code></pre>
<p>As expected, because little to no bias was observed, the uncalibrated and calibrated p-values are very similar.</p>
</div>
<div id="confidence-interval-calibration-1" class="section level3">
<h3><span class="header-section-number">19.3.5</span> Confidence interval calibration</h3>
<p>Similarly, we can use the estimates for our negative and positive controls to calibrate the confidence intervals. The Shiny app automatically reports the calibrate confidence intervals. In R we can calibrate intervals using the <code>fitSystematicErrorModel</code> and <code>calibrateConfidenceInterval</code> functions in the <a href="https://ohdsi.github.io/EmpiricalCalibration/">EmpiricalCalibration</a> package, as described in detail in the <a href="https://ohdsi.github.io/EmpiricalCalibration/articles/EmpiricalCiCalibrationVignette.html">“Empirical calibration of confidence intervals” vignette</a>.</p>
<p>Before calibration, the estimated hazard ratios (95% confidence interval) are 4.32 (2.45 - 8.08) and 1.13 (0.59 - 2.18), for angioedema and AMI respectively. The calibrated hazard ratios are 4.75 (2.52 - 9.04) and 1.15 (0.58 - 2.30).</p>
</div>
<div id="between-database-heterogeneity" class="section level3">
<h3><span class="header-section-number">19.3.6</span> Between-database heterogeneity</h3>
<p>Just as we executed our analysis on one database, in this case the IBM MarketScan Medicaid (MDCD) database, we can also run our analysis on other databases that adhere to the Common Data Model (CDM). Figure <a href="MethodValidity.html#fig:forest">19.7</a> shows the forest plot and meta-analytic estimates (assuming random effects) <span class="citation">(DerSimonian and Laird <a href="#ref-dersimonian_1986">1986</a>)</span> across a total of five databases for the outcome of angioedema. This figure was generated using the <code>plotMetaAnalysisForest</code> function in the <a href="https://ohdsi.github.io/EvidenceSynthesis/">EvidenceSynthesis</a> package.</p>
<div class="figure" style="text-align: center"><span id="fig:forest"></span>
<img src="images/MethodValidity/forest.png" alt="Effect size estimates and 95% confidence intervals (CI) from five different databases and a meta-analytic estimatewhen comparing ACE inhibitors to thiazides and thiazide-like diuretics for the risk of angioedema." width="90%" />
<p class="caption">
Figure 19.7: Effect size estimates and 95% confidence intervals (CI) from five different databases and a meta-analytic estimatewhen comparing ACE inhibitors to thiazides and thiazide-like diuretics for the risk of angioedema.
</p>
</div>
<p>Although all confidence intervals are above one, suggesting agreement on the fact that there is an effect, the <span class="math inline">\(I^2\)</span> suggests between-database heterogeneity. However, if we compute the <span class="math inline">\(I^2\)</span> using the calibrated confidence intervals as shown in Figure <a href="MethodValidity.html#fig:forestCal">19.8</a>, we see that this heterogeneity can be explained by the bias observed in each database through the negative and positive controls. The empirical calibration appears to properly taken this bias into account.</p>
<div class="figure" style="text-align: center"><span id="fig:forestCal"></span>
<img src="images/MethodValidity/forestCal.png" alt="Calibrated Effect size estimates and 95% confidence intervals (CI) from five different databases and a meta-analytic estimate for the hazard ratio of angioedema when comparing ACE inhibitors to thiazides and thiazide-like diuretics." width="90%" />
<p class="caption">
Figure 19.8: Calibrated Effect size estimates and 95% confidence intervals (CI) from five different databases and a meta-analytic estimate for the hazard ratio of angioedema when comparing ACE inhibitors to thiazides and thiazide-like diuretics.
</p>
</div>
</div>
<div id="sensitivity-analyses-1" class="section level3">
<h3><span class="header-section-number">19.3.7</span> Sensitivity analyses</h3>
<p>One of the design choices in our analysis was to use variable-ratio matching on the propensity score. However, we could have also used stratification on the propensity score. Because we we are uncertain about this choice, we may decide to use both. Table <a href="MethodValidity.html#tab:sensAnalysis">19.2</a> shows the effect size estimates for AMI and angioedema, both calibrated and uncalibrated, when using variable-ratio matching and stratification (10 equally-sized strata).</p>
<table style="width:86%;">
<caption><span id="tab:sensAnalysis">Table 19.2: </span> Uncalibrated and calibrated hazard ratios (95% confidence interval) for the two analysis variants.</caption>
<colgroup>
<col width="30%" />
<col width="16%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Outcome</th>
<th align="left">Adjustment</th>
<th>Uncalibrated</th>
<th>Calibrated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Angioedema</td>
<td align="left">Matching</td>
<td>4.32 (2.45 - 8.08)</td>
<td>4.75 (2.52 - 9.04)</td>
</tr>
<tr class="even">
<td align="left">Angioedema</td>
<td align="left">Stratification</td>
<td>4.57 (3.00 - 7.19)</td>
<td>4.52 (2.85 - 7.19)</td>
</tr>
<tr class="odd">
<td align="left">Acute myocardial infarction</td>
<td align="left">Matching</td>
<td>1.13 (0.59 - 2.18)</td>
<td>1.15 (0.58 - 2.30)</td>
</tr>
<tr class="even">
<td align="left">Acute myocardial infarction</td>
<td align="left">Stratification</td>
<td>1.43 (1.02 - 2.06)</td>
<td>1.45 (1.03 - 2.06)</td>
</tr>
</tbody>
</table>
<p>We see that the estimates from the matched and stratified analysis are in strong agreement, with the confidence intervals for stratification falling completely inside of the confidence intervals for matching. This suggests that our uncertainty around this design choice does not impact the validity of our estimates. Stratification does appear to give us more power (narrower confidence intervals), which is not surprising since matching results in loss of data, whereas stratification does not. The price for this could be an increase in bias, due to within-strata residual confounding, although we see no evidence of increased bias reflected in the calibrated confidence intervals.</p>

<div class="rmdimportant">
Study diagnostics allow us to evaluate design choices even before fully executing a study. It is recommended not to finalize the protocol before generating and reviewing all study diagnostics. To avoid p-hacking (adjusting the design to achieve a desired result), this should be done before estimating the effect of interest.
</div>

</div>
</div>
<div id="ohdsi-methods-benchmark" class="section level2">
<h2><span class="header-section-number">19.4</span> OHDSI Methods Benchmark</h2>
<p>Although the recommended practice is to empirically evaluate a method’s performance within the context that it is applied, using negative and positive controls that are in ways similar to the exposures-outcomes pairs of interest (for example using the same exposure or the same outcome), there is also value in evaluating a method’s performance in general. This is why the OHDSI Methods Evaluation Benchmark was developed. The benchmark evaluates performance using a wide range of control questions, including those with chronic or acute outcomes, and long-term or short-term exposures. The results on this benchmark can help demonstrate the overall userfulness of a method, and can be used as a prior belief about the performance of a method when a context-specific empirical evaluation is not (yet) available. The benchmark consists of 200 carefully selected negative controls that can be stratified into eight categories, with the controls in each category either sharing the same exposure or the same outcome. From these 200 negative controls, 600 synthetic positive controls are derived as described in Section <a href="MethodValidity.html#PositiveControls">19.2.2</a>. To evaluate a method, it must be used to produce effect size estimates for all controls, after which the metrics described in Section <a href="MethodValidity.html#metrics">19.2.3</a> can be computed. The benchmark is publicly available, and can be deployed as described in the <a href="https://ohdsi.github.io/MethodEvaluation/articles/OhdsiMethodsBenchmark.html">Running the OHDSI Methods Benchmark vignette</a> in the <a href="https://ohdsi.github.io/MethodEvaluation/">MethodEvaluation</a> package.</p>
<p>We have run all the methods in the OHDSI Methods Library through this benchmark, with various analysis choices per method. For example, the cohort method was evaluated using propensity score matching, stratification, and weighting. This experiment was executed on four large observational healthcare databases. The results, viewable in an online Shiny app<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>, show that although several methods show high AUC (the ability to distinguish positive controls from negative controls), most methods in most settings demonstrate high type 1 error and low coverage of the 95% confidence interval, as shown in Figure <a href="MethodValidity.html#fig:methodEval">19.9</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:methodEval"></span>
<img src="images/MethodValidity/methodEval.png" alt="Coverage of the 95% confidence interval for the methods in the Methods Library. Each dot represents the performance of a specific set of analysis choices. The dashed line indicates nominal performance (95% coverage). SCCS = Self-Controlled Case Series, GI = Gastrointestinal, IBD = inflammatory bowel disease." width="100%" />
<p class="caption">
Figure 19.9: Coverage of the 95% confidence interval for the methods in the Methods Library. Each dot represents the performance of a specific set of analysis choices. The dashed line indicates nominal performance (95% coverage). SCCS = Self-Controlled Case Series, GI = Gastrointestinal, IBD = inflammatory bowel disease.
</p>
</div>
<p>This emphasizes the need for empirical evaluation and calibration: if no empirical evaluation is performed, wich is true for almost all published observational studies, we must assume a prior informed by the results in Figure <a href="MethodValidity.html#fig:methodEval">19.9</a>, and conclude that it is likely that the true effect size is not contained in the 95% confidence interval!</p>
<p>Our evaluation of the designs in the Methods Library also shows that empirical calibration restores type 1 error and coverage to their nominal values, although often at the cost of increasing type 2 error and descreasing precision.</p>
</div>
<div id="summary-5" class="section level2">
<h2><span class="header-section-number">19.5</span> Summary</h2>

<div class="rmdsummary">
<ul>
<li><p>A method’s validity depends on whether the assumptions underlying the method are met.</p></li>
<li><p>Where possible, these assumptions should be empirically tested using study diagnostics.</p></li>
<li><p>Control hypotheses, questions where the answer is known, should be used to evaluate whether a specific study design produces answers in line with the truth.</p></li>
<li><p>Often, p-values and confidence intervals do not demonstrate nominal characteristics as measured using control hypotheses.</p></li>
<li><p>These characteristics can often be restored to nominal using empirical calibration.</p></li>
<li>Study diagnostics can be used to guide analytic design choices and adapt the protocol, as long as the researcher remains blinded to the effect of interest to avoid p-hacking.</li>
</ul>
</div>

</div>
<div id="exercises-3" class="section level2">
<h2><span class="header-section-number">19.6</span> Exercises</h2>
<p>Todo</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-arnold_2016">
<p>Arnold, B. F., A. Ercumen, J. Benjamin-Chung, and J. M. Colford. 2016. “Brief Report: Negative Controls to Detect Selection Bias and Measurement Bias in Epidemiologic Studies.” <em>Epidemiology</em> 27 (5): 637–41.</p>
</div>
<div id="ref-dersimonian_1986">
<p>DerSimonian, R., and N. Laird. 1986. “Meta-analysis in clinical trials.” <em>Control Clin Trials</em> 7 (3): 177–88.</p>
</div>
<div id="ref-higgins_2003">
<p>Higgins, J. P., S. G. Thompson, J. J. Deeks, and D. G. Altman. 2003. “Measuring inconsistency in meta-analyses.” <em>BMJ</em> 327 (7414): 557–60.</p>
</div>
<div id="ref-lipsitch_2010">
<p>Lipsitch, M., E. Tchetgen Tchetgen, and T. Cohen. 2010. “Negative controls: a tool for detecting confounding and bias in observational studies.” <em>Epidemiology</em> 21 (3): 383–88.</p>
</div>
<div id="ref-madigan_2013">
<p>Madigan, D., P. B. Ryan, M. Schuemie, P. E. Stang, J. M. Overhage, A. G. Hartzema, M. A. Suchard, W. DuMouchel, and J. A. Berlin. 2013. “Evaluating the impact of database heterogeneity on observational study results.” <em>Am. J. Epidemiol.</em> 178 (4): 645–51.</p>
</div>
<div id="ref-noren_2014">
<p>Noren, G. N., O. Caster, K. Juhlin, and M. Lindquist. 2014. “Zoo or savannah? Choice of training ground for evidence-based pharmacovigilance.” <em>Drug Saf</em> 37 (9): 655–59.</p>
</div>
<div id="ref-prased_2013">
<p>Prasad, V., and A. B. Jena. 2013. “Prespecified falsification end points: can they validate true observational associations?” <em>JAMA</em> 309 (3): 241–42.</p>
</div>
<div id="ref-rosenbaum_2005">
<p>Rosenbaum, P. 2005. “Sensitivity Analysis in Observational Studies.” In <em>Encyclopedia of Statistics in Behavioral Science</em>. American Cancer Society. doi:<a href="https://doi.org/10.1002/0470013192.bsa606">10.1002/0470013192.bsa606</a>.</p>
</div>
<div id="ref-schuemie_2018">
<p>Schuemie, M. 2018. “Empirical confidence interval calibration for population-level effect estimation studies in observational healthcare data.” <em>Proc. Natl. Acad. Sci. U.S.A.</em> 115 (11): 2571–7.</p>
</div>
<div id="ref-schuemie_2014">
<p>Schuemie, M. J., P. B. Ryan, W. DuMouchel, M. A. Suchard, and D. Madigan. 2014. “Interpreting observational studies: why empirical calibration is needed to correct p-values.” <em>Stat Med</em> 33 (2): 209–18.</p>
</div>
<div id="ref-schuemie_2018b">
<p>Schuemie, M. J., P. B. Ryan, G. Hripcsak, D. Madigan, and M. A. Suchard. 2018. “Improving reproducibility by using high-throughput observational studies with empirical calibration.” <em>Philos Trans A Math Phys Eng Sci</em> 376 (2128).</p>
</div>
<div id="ref-suchard_2013">
<p>Suchard, M. A., S. E. Simpson, Ivan Zorych, P. B. Ryan, and David Madigan. 2013. “Massive Parallelization of Serial Inference Algorithms for a Complex Generalized Linear Model.” <em>ACM Trans. Model. Comput. Simul.</em> 23 (1). New York, NY, USA: ACM: 10:1–10:17. doi:<a href="https://doi.org/10.1145/2414416.2414791">10.1145/2414416.2414791</a>.</p>
</div>
<div id="ref-voss_2016">
<p>Voss, E. A., R. D. Boyce, P. B. Ryan, J. van der Lei, P. R. Rijnbeek, and M. J. Schuemie. 2016. “Accuracy of an Automated Knowledge Base for Identifying Drug Adverse Reactions.” <em>J Biomed Inform</em>, December.</p>
</div>
<div id="ref-zaadstra_2008">
<p>Zaadstra, B. M., A. M. Chorus, S. van Buuren, H. Kalsbeek, and J. M. van Noort. 2008. “Selective association of multiple sclerosis with infectious mononucleosis.” <em>Mult. Scler.</em> 14 (3): 307–13.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p><a href="http://data.ohdsi.org/MethodEvalViewer/" class="uri">http://data.ohdsi.org/MethodEvalViewer/</a><a href="MethodValidity.html#fnref15">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="SoftwareValidity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="StudySteps.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/OHDSI/TheBookOfOhdsi/edit/master/MethodValidity.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["TheBookOfOhdsi.pdf", "TheBookOfOhdsi.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
